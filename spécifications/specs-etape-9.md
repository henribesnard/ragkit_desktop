# ğŸ§° RAGKIT Desktop â€” SpÃ©cifications Ã‰tape 9 : LLM / GÃ©nÃ©ration

> **Ã‰tape** : 9 â€” LLM / GÃ©nÃ©ration  
> **Tag cible** : `v0.10.0`  
> **Date** : 17 fÃ©vrier 2026  
> **DÃ©pÃ´t** : https://github.com/henribesnard/ragkit_desktop.git  
> **PrÃ©requis** : Ã‰tape 8 (Reranking) implÃ©mentÃ©e et validÃ©e

---

## 1. Objectif

Ajouter la **gÃ©nÃ©ration de rÃ©ponses en langage naturel** par un LLM, en utilisant comme contexte les chunks rÃ©cupÃ©rÃ©s par le pipeline de retrieval (Ã‰tapes 5â€“8). C'est l'Ã©tape qui transforme RAGKIT d'un moteur de recherche en un **assistant conversationnel RAG complet**.

Cette Ã©tape livre :
- Une section `PARAMÃˆTRES > ParamÃ¨tres avancÃ©s > LLM / GÃ‰NÃ‰RATION` complÃ¨te et fonctionnelle.
- Quatre **providers LLM** : **OpenAI** (GPT-4o, GPT-4o-mini), **Anthropic** (Claude 3.5 Sonnet, Claude 3 Haiku), **Ollama** (modÃ¨les locaux : Llama 3, Mistral, Phi-3), **Mistral AI** (Mistral Large, Mistral Small).
- Le **streaming** des rÃ©ponses token par token dans le chat.
- L'**assemblage du contexte** : sÃ©lection et formatage des chunks en contexte pour le prompt LLM, avec respect du budget token.
- Les **citations de sources** : chaque affirmation de la rÃ©ponse peut Ãªtre liÃ©e au chunk source, avec format configurable (inline ou footnote).
- Un **prompt systÃ¨me par dÃ©faut** adaptÃ© au RAG, entiÃ¨rement personnalisable.
- Le **comportement d'incertitude** : quand le LLM ne trouve pas la rÃ©ponse dans le contexte, il le dit honnÃªtement.
- L'ajout de paramÃ¨tres LLM dans les **ParamÃ¨tres gÃ©nÃ©raux** (modÃ¨le, tempÃ©rature, langue, sources).
- Un **mode debug** montrant le prompt complet envoyÃ© au LLM, les chunks injectÃ©s, et les latences.

**Pas d'historique de conversation** Ã  cette Ã©tape. Chaque requÃªte est indÃ©pendante (pas de mÃ©moire entre les messages). L'historique conversationnel sera ajoutÃ© Ã  l'Ã‰tape 10 (Agents & Orchestration).

---

## 2. SpÃ©cifications fonctionnelles

### 2.1 Section PARAMÃˆTRES > ParamÃ¨tres avancÃ©s > LLM / GÃ‰NÃ‰RATION

#### Structure de l'onglet PARAMÃˆTRES Ã  cette Ã©tape

```
PARAMÃˆTRES
â”œâ”€â”€ ParamÃ¨tres gÃ©nÃ©raux
â”‚   â”œâ”€â”€ Mode d'ingestion (Manuel / Automatique)          â† Ã‰tape 4
â”‚   â”œâ”€â”€ Type de recherche (SÃ©mantique / Lexicale / Hybride)  â† Ã‰tape 7
â”‚   â”œâ”€â”€ ModÃ¨le LLM                                       â† NOUVEAU
â”‚   â”œâ”€â”€ TempÃ©rature                                       â† NOUVEAU
â”‚   â””â”€â”€ Langue de rÃ©ponse                                 â† NOUVEAU
â””â”€â”€ ParamÃ¨tres avancÃ©s
    â”œâ”€â”€ INGESTION & PRÃ‰PROCESSING                         â† Ã‰tape 1
    â”œâ”€â”€ CHUNKING                                          â† Ã‰tape 2
    â”œâ”€â”€ EMBEDDING                                         â† Ã‰tape 3
    â”œâ”€â”€ BASE DE DONNÃ‰ES VECTORIELLE                       â† Ã‰tape 4
    â”œâ”€â”€ RECHERCHE SÃ‰MANTIQUE                              â† Ã‰tape 5
    â”œâ”€â”€ RECHERCHE LEXICALE                                â† Ã‰tape 6
    â”œâ”€â”€ RECHERCHE HYBRIDE                                 â† Ã‰tape 7
    â”œâ”€â”€ RERANKING                                         â† Ã‰tape 8
    â””â”€â”€ LLM / GÃ‰NÃ‰RATION                                  â† NOUVEAU
```

#### Layout de la section LLM / GÃ‰NÃ‰RATION

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM / GÃ‰NÃ‰RATION                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ Provider et modÃ¨le â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ â”‚
â”‚  â”‚  â”‚ â˜ï¸ OpenAI     â”‚ â”‚ â˜ï¸ Anthropic  â”‚ â”‚ ğŸ¦™ Ollama    â”‚       â”‚ â”‚
â”‚  â”‚  â”‚              â”‚ â”‚              â”‚ â”‚ (local)      â”‚       â”‚ â”‚
â”‚  â”‚  â”‚ âœ“ SÃ‰LECT.   â”‚ â”‚              â”‚ â”‚              â”‚       â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚ â”‚
â”‚  â”‚  â”‚ â˜ï¸ Mistral AI â”‚                                          â”‚ â”‚
â”‚  â”‚  â”‚              â”‚                                          â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  ModÃ¨le : [â–¾ gpt-4o-mini                            ]     â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€ Fiche modÃ¨le â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚  ğŸ“ Contexte : 128K tokens                           â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  ğŸŒ Langues : Multilingue                            â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  ğŸ’° CoÃ»t : ~$0.15 / 1M input Â· $0.60 / 1M output   â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  âš¡ Latence : ~500-1500 ms (premier token)           â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  ClÃ© API OpenAI : [â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢] [ğŸ‘] [âœ“ Valide]     â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  [ğŸ”Œ Tester la connexion]                                  â”‚ â”‚
â”‚  â”‚  âœ… Connexion rÃ©ussie â€” gpt-4o-mini Â· 842 ms              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ ParamÃ¨tres de gÃ©nÃ©ration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  TempÃ©rature :      [â—†=========] 0.1                       â”‚ â”‚
â”‚  â”‚  Max tokens :       [====â—†=====] 2000                      â”‚ â”‚
â”‚  â”‚  Top P :            [========â—†=] 0.9                       â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â„¹ï¸ TempÃ©rature basse (0.0â€“0.3) = rÃ©ponses factuelles.     â”‚ â”‚
â”‚  â”‚  TempÃ©rature Ã©levÃ©e (0.7+) = plus de crÃ©ativitÃ© mais       â”‚ â”‚
â”‚  â”‚  risque accru d'hallucinations.                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ Comportement et citations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â˜‘ Citer les sources dans la rÃ©ponse                      â”‚ â”‚
â”‚  â”‚  Format des citations : (â€¢) Inline [Source: nom]          â”‚ â”‚
â”‚  â”‚                          ( ) Footnote Â¹Â²Â³                 â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â˜‘ Admettre l'incertitude                                 â”‚ â”‚
â”‚  â”‚  Phrase d'incertitude :                                    â”‚ â”‚
â”‚  â”‚  [Je n'ai pas trouvÃ© cette information dans les documents â”‚ â”‚
â”‚  â”‚   disponibles.                                        ]   â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  Langue de rÃ©ponse : [â–¾ auto (mÃªme langue que la requÃªte)]â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ Contexte (chunks envoyÃ©s au LLM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  Nombre max de chunks :  [==â—†=======] 5                    â”‚ â”‚
â”‚  â”‚  Tokens max de contexte : [====â—†=====] 4000                â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â„¹ï¸ Les chunks les plus pertinents (aprÃ¨s recherche et      â”‚ â”‚
â”‚  â”‚  reranking) sont injectÃ©s dans le prompt. Un contexte      â”‚ â”‚
â”‚  â”‚  plus large donne plus d'information au LLM mais coÃ»te    â”‚ â”‚
â”‚  â”‚  plus cher et peut diluer la pertinence.                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ Prompt systÃ¨me â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚ Tu es un assistant spÃ©cialisÃ© qui rÃ©pond aux         â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ questions en se basant UNIQUEMENT sur le contexte    â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ fourni. Cite tes sources. Si tu ne trouves pas       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ l'information, dis-le honnÃªtement.                   â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                                                      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚ Lignes: 6 Â· Tokens: ~85                              â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  [â†» Restaurer le prompt par dÃ©faut]                        â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â„¹ï¸ Le prompt systÃ¨me dÃ©finit le comportement du LLM.      â”‚ â”‚
â”‚  â”‚  Modifiez-le pour adapter le ton, le format de rÃ©ponse    â”‚ â”‚
â”‚  â”‚  ou les consignes spÃ©cifiques Ã  votre domaine.            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â–¸ ParamÃ¨tres avancÃ©s                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Timeout (secondes) : [====â—†=====] 60                      â”‚ â”‚
â”‚  â”‚  Max retries :        [â—†=========] 2                       â”‚ â”‚
â”‚  â”‚  â˜‘ Streaming activÃ©                                        â”‚ â”‚
â”‚  â”‚  â˜ Mode debug activÃ© par dÃ©faut                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  [â†» RÃ©initialiser au profil]                                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ParamÃ¨tres gÃ©nÃ©raux â€” extension LLM

Trois nouveaux paramÃ¨tres sont ajoutÃ©s dans `PARAMÃˆTRES > ParamÃ¨tres gÃ©nÃ©raux` :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PARAMÃˆTRES GÃ‰NÃ‰RAUX                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Mode d'ingestion ........ (â€¢) Manuel  ( ) Automatique  â† Ã‰t.4 â”‚
â”‚  Type de recherche ....... (â€¢) Hybride              â† Ã‰t.7     â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                                                                 â”‚
â”‚  ModÃ¨le LLM                                         â† NOUVEAU  â”‚
â”‚  [â–¾ gpt-4o-mini (OpenAI)                                   ]   â”‚
â”‚                                                                 â”‚
â”‚  TempÃ©rature                                         â† NOUVEAU  â”‚
â”‚  Factuel â—€ [â—†=========] â–¶ CrÃ©atif                              â”‚
â”‚                0.1                                              â”‚
â”‚                                                                 â”‚
â”‚  Langue de rÃ©ponse                                   â† NOUVEAU  â”‚
â”‚  [â–¾ Auto (mÃªme langue que la question)                     ]   â”‚
â”‚                                                                 â”‚
â”‚  â„¹ï¸ Ces paramÃ¨tres contrÃ´lent le comportement gÃ©nÃ©ral du         â”‚
â”‚  chat. Pour un rÃ©glage fin, utilisez ParamÃ¨tres avancÃ©s >      â”‚
â”‚  LLM / GÃ‰NÃ‰RATION.                                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Le CHAT â€” interface conversationnelle complÃ¨te

L'Ã‰tape 9 transforme le CHAT d'un afficheur de rÃ©sultats bruts en un **chat conversationnel complet** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ’¬ CHAT                                           [âš™ Options] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ Zone de conversation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  ğŸ‘¤ Quelles sont les conditions de rÃ©siliation du contrat? â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  ğŸ¤– D'aprÃ¨s les documents disponibles, les conditions de   â”‚ â”‚
â”‚  â”‚  rÃ©siliation sont dÃ©finies Ã  l'article 12 du contrat de   â”‚ â”‚
â”‚  â”‚  service. Le contrat peut Ãªtre rÃ©siliÃ© dans les cas        â”‚ â”‚
â”‚  â”‚  suivants :                                                â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â€¢ **RÃ©siliation pour faute** : en cas de manquement      â”‚ â”‚
â”‚  â”‚    grave aux obligations contractuelles, avec un dÃ©lai     â”‚ â”‚
â”‚  â”‚    de mise en demeure de 30 jours.                         â”‚ â”‚
â”‚  â”‚    [Source: contrat-service-2024.pdf, p.8]                â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â€¢ **RÃ©siliation pour convenance** : avec un prÃ©avis de   â”‚ â”‚
â”‚  â”‚    3 mois et le paiement des prestations en cours.        â”‚ â”‚
â”‚  â”‚    [Source: contrat-service-2024.pdf, p.9]                â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â€¢ **RÃ©siliation de plein droit** : en cas de liquidation â”‚ â”‚
â”‚  â”‚    judiciaire ou de cessation d'activitÃ©.                 â”‚ â”‚
â”‚  â”‚    [Source: CGV-2024.pdf, p.3]                            â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â”€â”€ Sources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚  â”‚  ğŸ“„ contrat-service-2024.pdf Â· Pages 8-9 Â· Score 0.987   â”‚ â”‚
â”‚  â”‚  ğŸ“„ CGV-2024.pdf Â· Page 3 Â· Score 0.934                  â”‚ â”‚
â”‚  â”‚  ğŸ“„ avenant-2023.pdf Â· Page 2 Â· Score 0.891              â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  [â–¾ ğŸ”€ Hybride â–¾]                                              â”‚
â”‚  [Posez votre question...                              ] [â†’]   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Ã‰lÃ©ments de la rÃ©ponse

| Ã‰lÃ©ment | Description |
|---------|-------------|
| **Texte de la rÃ©ponse** | Texte gÃ©nÃ©rÃ© par le LLM en markdown, avec mise en forme (gras, listes, etc.) |
| **Citations inline** | `[Source: nom_doc, p.N]` insÃ©rÃ©es dans le texte aux endroits appropriÃ©s |
| **Panneau Sources** | Liste des chunks utilisÃ©s comme contexte, triÃ©s par score, cliquables pour voir le chunk complet |
| **Indicateur de confiance** | Si le LLM exprime une incertitude, le message est annotÃ© avec un badge âš ï¸ |

### 2.4 Streaming des rÃ©ponses

La rÃ©ponse du LLM est affichÃ©e **token par token** en streaming :

**Comportement** :
1. L'utilisateur soumet une requÃªte â†’ apparition d'un indicateur de chargement "Recherche en cours..."
2. Le pipeline de retrieval s'exÃ©cute (recherche + reranking) â†’ l'indicateur passe Ã  "GÃ©nÃ©ration en cours..."
3. Les tokens commencent Ã  arriver du LLM â†’ affichage incrÃ©mental dans la bulle de rÃ©ponse.
4. Le streaming se termine â†’ le panneau Sources apparaÃ®t sous la rÃ©ponse.

**Ã‰tat de l'interface pendant le streaming** :
- Le bouton d'envoi est remplacÃ© par un bouton "â¹ ArrÃªter" qui interrompt la gÃ©nÃ©ration.
- Le texte s'affiche progressivement, le markdown est rendu en temps rÃ©el.
- Les citations ne sont rendues cliquables qu'une fois le streaming terminÃ©.

### 2.5 Assemblage du contexte

Le composant **Context Assembler** sÃ©lectionne et formate les chunks en contexte pour le prompt LLM :

```
RÃ©sultats du retrieval (aprÃ¨s reranking)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONTEXT ASSEMBLER                   â”‚
â”‚                                      â”‚
â”‚  1. SÃ©lection : top context_max_chunksâ”‚
â”‚  2. Budget tokens : â‰¤ context_max_tokensâ”‚
â”‚  3. Formatage : chaque chunk avec    â”‚
â”‚     source, page, score              â”‚
â”‚  4. Injection dans le prompt systÃ¨me â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Prompt complet = system_prompt + context + user_query
```

**Format du contexte injectÃ©** :

```
<context>
<source id="1" title="contrat-service-2024.pdf" page="8" score="0.987">
Les conditions de rÃ©siliation anticipÃ©e sont dÃ©finies Ã  l'article 12
du prÃ©sent contrat. Le prestataire peut rÃ©silier le contrat avec un
prÃ©avis de 30 jours en cas de manquement grave...
</source>
<source id="2" title="CGV-2024.pdf" page="3" score="0.934">
Article 7 â€” RÃ©siliation. Le contrat peut Ãªtre rÃ©siliÃ© de plein droit
en cas de liquidation judiciaire ou de cessation d'activitÃ©...
</source>
<source id="3" title="avenant-2023.pdf" page="2" score="0.891">
L'avenant modifie les conditions de rÃ©siliation pour convenance
en ajoutant un prÃ©avis de 3 mois...
</source>
</context>
```

**RÃ¨gles de sÃ©lection** :
1. Les chunks sont triÃ©s par score (post-reranking ou post-recherche).
2. On ajoute les chunks un par un jusqu'Ã  atteindre `context_max_chunks` ou `context_max_tokens`.
3. Si un chunk dÃ©passe le budget token restant, il est tronquÃ©.
4. Les chunks sont formatÃ©s en XML avec mÃ©tadonnÃ©es (titre, page, score) pour permettre au LLM de les citer.

### 2.6 Prompt systÃ¨me par dÃ©faut

```
Tu es un assistant spÃ©cialisÃ© dans l'analyse de documents. Tu rÃ©ponds aux questions
en te basant UNIQUEMENT sur le contexte fourni entre les balises <context> et </context>.

RÃ¨gles :
1. Base ta rÃ©ponse exclusivement sur le contexte fourni. Ne gÃ©nÃ¨re jamais d'information
   qui ne s'y trouve pas.
2. Cite tes sources en utilisant le format [Source: titre, p.N] aprÃ¨s chaque affirmation
   importante.
3. Si l'information demandÃ©e n'est pas dans le contexte, dis-le honnÃªtement en utilisant
   la phrase : "{uncertainty_phrase}".
4. RÃ©ponds dans la langue de la question, sauf indication contraire.
5. Structure ta rÃ©ponse de maniÃ¨re claire avec des paragraphes, listes ou titres si
   nÃ©cessaire.
6. Si plusieurs sources se contredisent, signale-le explicitement.
```

Les variables `{uncertainty_phrase}` et `{citation_format}` sont remplacÃ©es dynamiquement.

### 2.7 Panneau Sources (sous la rÃ©ponse)

Chaque rÃ©ponse est accompagnÃ©e d'un panneau **Sources** montrant les chunks utilisÃ©s comme contexte :

```
â”Œâ”€â”€ Sources (3 documents) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚  ğŸ“„ contrat-service-2024.pdf Â· Pages 8-9 Â· Score 0.987      â”‚
â”‚  â–¸ Cliquer pour voir l'extrait                              â”‚
â”‚                                                              â”‚
â”‚  ğŸ“„ CGV-2024.pdf Â· Page 3 Â· Score 0.934                     â”‚
â”‚  â–¸ Cliquer pour voir l'extrait                              â”‚
â”‚                                                              â”‚
â”‚  ğŸ“„ avenant-2023.pdf Â· Page 2 Â· Score 0.891                 â”‚
â”‚  â–¸ Cliquer pour voir l'extrait                              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Au clic** sur une source, l'extrait du chunk s'affiche en accordÃ©on avec les mÃ©tadonnÃ©es complÃ¨tes.

### 2.8 Mode debug enrichi

Le mode debug de l'Ã‰tape 9 est le plus complet. Il montre l'ensemble du pipeline :

```
â”Œâ”€â”€ Mode debug â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  Pipeline : Hybride (Î±=0.50, RRF) â†’ Reranking â†’ LLM (GPT-4o-mini)â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Retrieval â”€â”€                                                â”‚
â”‚  Recherche : 287 ms Â· Reranking : 255 ms Â· 5 chunks retenus    â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ Contexte â”€â”€                                                 â”‚
â”‚  Chunks injectÃ©s : 5 Â· Tokens contexte : 2 847 / 4 000        â”‚
â”‚  Sources : contrat-service (p.8, p.9), CGV (p.3), avenant (p.2)â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ GÃ©nÃ©ration â”€â”€                                               â”‚
â”‚  ModÃ¨le : gpt-4o-mini Â· TempÃ©rature : 0.1                     â”‚
â”‚  Tokens prompt : 3 142 Â· Tokens rÃ©ponse : 487                  â”‚
â”‚  Time to first token : 623 ms Â· Temps total : 2 847 ms        â”‚
â”‚  CoÃ»t estimÃ© : ~$0.0005 input + $0.0003 output                â”‚
â”‚                                                                 â”‚
â”‚  â–¸ Voir le prompt complet (system + context + query)           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Providers LLM

### 3.1 Catalogue des providers

| Provider | ClÃ© | Authentification | ModÃ¨les | Streaming | Latence |
|----------|-----|:---:|---------|:---:|:---:|
| OpenAI | `openai` | API Key | GPT-4o, GPT-4o-mini, GPT-4-turbo | âœ… | 500-1500 ms |
| Anthropic | `anthropic` | API Key | Claude 3.5 Sonnet, Claude 3 Haiku | âœ… | 500-2000 ms |
| Ollama | `ollama` | Aucune | Llama 3, Mistral, Phi-3, Gemma 2 | âœ… | 1000-5000 ms |
| Mistral AI | `mistral` | API Key | Mistral Large, Mistral Small, Codestral | âœ… | 500-1500 ms |

### 3.2 Catalogue des modÃ¨les

#### OpenAI

| ModÃ¨le | ClÃ© | Contexte | CoÃ»t (input/output) | QualitÃ© |
|--------|-----|:---:|:---:|:---:|
| GPT-4o | `gpt-4o` | 128K | $2.50 / $10.00 par 1M | â­â­â­â­â­ |
| GPT-4o mini | `gpt-4o-mini` | 128K | $0.15 / $0.60 par 1M | â­â­â­â­ |
| GPT-4 Turbo | `gpt-4-turbo` | 128K | $10 / $30 par 1M | â­â­â­â­â­ |

#### Anthropic

| ModÃ¨le | ClÃ© | Contexte | CoÃ»t (input/output) | QualitÃ© |
|--------|-----|:---:|:---:|:---:|
| Claude 3.5 Sonnet | `claude-3-5-sonnet-20241022` | 200K | $3.00 / $15.00 par 1M | â­â­â­â­â­ |
| Claude 3 Haiku | `claude-3-haiku-20240307` | 200K | $0.25 / $1.25 par 1M | â­â­â­â­ |

#### Ollama (local)

| ModÃ¨le | ClÃ© | Contexte | CoÃ»t | QualitÃ© |
|--------|-----|:---:|:---:|:---:|
| Llama 3.1 8B | `llama3.1:8b` | 128K | ğŸ†“ Gratuit | â­â­â­ |
| Mistral 7B | `mistral:7b` | 32K | ğŸ†“ Gratuit | â­â­â­ |
| Phi-3 Mini | `phi3:mini` | 128K | ğŸ†“ Gratuit | â­â­â­ |
| Gemma 2 9B | `gemma2:9b` | 8K | ğŸ†“ Gratuit | â­â­â­ |

#### Mistral AI

| ModÃ¨le | ClÃ© | Contexte | CoÃ»t (input/output) | QualitÃ© |
|--------|-----|:---:|:---:|:---:|
| Mistral Large | `mistral-large-latest` | 128K | $2.00 / $6.00 par 1M | â­â­â­â­â­ |
| Mistral Small | `mistral-small-latest` | 128K | $0.20 / $0.60 par 1M | â­â­â­â­ |

### 3.3 Interface unifiÃ©e des providers

Tous les providers implÃ©mentent la mÃªme interface unifiÃ©e, avec les diffÃ©rences d'API encapsulÃ©es :

| Aspect | OpenAI | Anthropic | Ollama | Mistral |
|--------|--------|-----------|--------|---------|
| **Endpoint** | `api.openai.com/v1/chat/completions` | `api.anthropic.com/v1/messages` | `localhost:11434/api/chat` | `api.mistral.ai/v1/chat/completions` |
| **Format messages** | `messages: [{role, content}]` | `messages: [{role, content}]` | `messages: [{role, content}]` | `messages: [{role, content}]` |
| **System prompt** | `messages[0].role = "system"` | `system: "..."` | `system: "..."` | `messages[0].role = "system"` |
| **Streaming** | SSE `data: {choices: [{delta}]}` | SSE `event: content_block_delta` | NDJSON `{message: {content}}` | SSE `data: {choices: [{delta}]}` |
| **Stop** | `n/a` (arrÃªt cÃ´tÃ© client) | `n/a` (arrÃªt cÃ´tÃ© client) | `n/a` (arrÃªt cÃ´tÃ© client) | `n/a` (arrÃªt cÃ´tÃ© client) |

---

## 4. Catalogue complet des paramÃ¨tres LLM / GÃ‰NÃ‰RATION

### 4.1 Provider et modÃ¨le

| ParamÃ¨tre | ClÃ© config | Type | DÃ©faut | Description |
|-----------|------------|------|--------|-------------|
| Provider | `llm.provider` | enum | Selon profil | `openai` \| `anthropic` \| `ollama` \| `mistral` |
| ModÃ¨le | `llm.model` | string | Selon profil | Identifiant du modÃ¨le |

### 4.2 ParamÃ¨tres de gÃ©nÃ©ration

| ParamÃ¨tre | ClÃ© config | Type | Min | Max | DÃ©faut | Description |
|-----------|------------|------|-----|-----|--------|-------------|
| TempÃ©rature | `llm.temperature` | float | 0.0 | 2.0 | Selon profil | CrÃ©ativitÃ©. 0 = factuel. RAG recommandÃ© : 0.0â€“0.3. |
| Max tokens | `llm.max_tokens` | int | 100 | 16384 | Selon profil | Longueur max de la rÃ©ponse en tokens |
| Top P | `llm.top_p` | float | 0.0 | 1.0 | Selon profil | Nucleus sampling. 0.9 = valeur standard RAG. |

### 4.3 Comportement et citations

| ParamÃ¨tre | ClÃ© config | Type | DÃ©faut | Description |
|-----------|------------|------|--------|-------------|
| Citer les sources | `llm.cite_sources` | bool | `true` | Inclure des citations dans la rÃ©ponse |
| Format des citations | `llm.citation_format` | enum | Selon profil | `inline` = `[Source: nom, p.N]`, `footnote` = exposants Â¹Â²Â³ avec notes en bas |
| Admettre l'incertitude | `llm.admit_uncertainty` | bool | `true` | Dire honnÃªtement quand l'info n'est pas trouvÃ©e |
| Phrase d'incertitude | `llm.uncertainty_phrase` | string | "Je n'ai pas trouvÃ© cette information dans les documents disponibles." | Message affichÃ© quand le LLM ne peut pas rÃ©pondre |
| Langue de rÃ©ponse | `llm.response_language` | enum | `auto` | `auto` \| `fr` \| `en`. `auto` = mÃªme langue que la requÃªte. |

### 4.4 Contexte

| ParamÃ¨tre | ClÃ© config | Type | Min | Max | DÃ©faut | Description |
|-----------|------------|------|-----|-----|--------|-------------|
| Max chunks | `llm.context_max_chunks` | int | 1 | 30 | Selon profil | Nombre max de chunks dans le contexte |
| Max tokens contexte | `llm.context_max_tokens` | int | 500 | 32000 | Selon profil | Budget token max pour le contexte |

### 4.5 Prompt systÃ¨me

| ParamÃ¨tre | ClÃ© config | Type | DÃ©faut | Description |
|-----------|------------|------|--------|-------------|
| Prompt systÃ¨me | `llm.system_prompt` | string | (voir Â§2.6) | Instructions pour le LLM. Variables : `{uncertainty_phrase}`, `{citation_format}` |

### 4.6 ParamÃ¨tres avancÃ©s

| ParamÃ¨tre | ClÃ© config | Type | Min | Max | DÃ©faut | Description |
|-----------|------------|------|-----|-----|--------|-------------|
| Timeout | `llm.timeout` | int | 10 | 300 | 60 | Timeout en secondes |
| Max retries | `llm.max_retries` | int | 0 | 5 | 2 | Nombre de tentatives en cas d'erreur |
| Streaming | `llm.streaming` | bool | â€” | â€” | `true` | Afficher les tokens progressivement |
| Debug | `llm.debug_default` | bool | â€” | â€” | `false` | Mode debug par dÃ©faut |

### 4.7 ParamÃ¨tres gÃ©nÃ©raux (extensions)

| ParamÃ¨tre | ClÃ© config | Type | DÃ©faut | Description |
|-----------|------------|------|--------|-------------|
| ModÃ¨le LLM | `general.llm_model` | string | Selon profil | Raccourci vers `llm.provider` + `llm.model` |
| TempÃ©rature | `general.llm_temperature` | float | Selon profil | Raccourci vers `llm.temperature` |
| Langue de rÃ©ponse | `general.response_language` | enum | `auto` | Raccourci vers `llm.response_language` |

**Synchronisation** : modifier ces paramÃ¨tres dans ParamÃ¨tres gÃ©nÃ©raux modifie aussi les paramÃ¨tres correspondants dans LLM / GÃ‰NÃ‰RATION, et inversement.

### 4.8 RÃ©sumÃ© des impacts

| ParamÃ¨tre | Impact principal | Impact secondaire |
|-----------|-----------------|-------------------|
| `provider` + `model` | **CRITIQUE** â€” QualitÃ© et coÃ»t de la gÃ©nÃ©ration | Latence, confidentialitÃ© (local vs cloud) |
| `temperature` | **IMPORTANT** â€” FactualitÃ© vs crÃ©ativitÃ©. 0 = dÃ©terministe, >0.5 = risque hallucinations | LÃ©gÃ¨re variation de latence |
| `max_tokens` | Longueur maximale des rÃ©ponses | CoÃ»t proportionnel aux tokens de sortie |
| `cite_sources` | TraÃ§abilitÃ© et vÃ©rifiabilitÃ© des rÃ©ponses | LÃ©ger overhead dans le prompt |
| `context_max_chunks` | QuantitÃ© d'information disponible pour le LLM | Plus de chunks = meilleure couverture mais "lost in the middle" |
| `context_max_tokens` | Budget token pour le contexte | Impact direct sur le coÃ»t |
| `system_prompt` | **FONDAMENTAL** â€” DÃ©finit le comportement et la personnalitÃ© | â€” |
| `streaming` | UX : rÃ©ponse progressive vs attente complÃ¨te | â€” |

---

## 5. Valeurs par dÃ©faut par profil

### 5.1 Matrice profil â†’ paramÃ¨tres LLM

| ParamÃ¨tre | `technical_documentation` | `faq_support` | `legal_compliance` | `reports_analysis` | `general` |
|-----------|:---:|:---:|:---:|:---:|:---:|
| `provider` | `openai` | `openai` | `openai` | `openai` | `openai` |
| `model` | `gpt-4o-mini` | `gpt-4o-mini` | `gpt-4o-mini` | `gpt-4o-mini` | `gpt-4o-mini` |
| `temperature` | 0.1 | 0.3 | 0.0 | 0.2 | 0.7 |
| `max_tokens` | 2000 | 1000 | 3000 | 2000 | 2000 |
| `top_p` | 0.9 | 0.95 | 0.85 | 0.9 | 0.95 |
| `cite_sources` | `true` | `true` | `true` | `true` | `true` |
| `citation_format` | `inline` | `inline` | `footnote` | `inline` | `inline` |
| `admit_uncertainty` | `true` | `true` | `true` | `true` | `true` |
| `response_language` | `auto` | `auto` | `auto` | `auto` | `auto` |
| `context_max_chunks` | 5 | 3 | 8 | 5 | 5 |
| `context_max_tokens` | 4000 | 2000 | 8000 | 4000 | 4000 |
| `timeout` | 60 | 30 | 60 | 60 | 60 |
| `streaming` | `true` | `true` | `true` | `true` | `true` |

### 5.2 Justification des choix

- **`technical_documentation` â†’ `temperature=0.1`** : les rÃ©ponses techniques doivent Ãªtre factuelles et prÃ©cises. Une tempÃ©rature trÃ¨s basse minimise les hallucinations tout en gardant un minimum de variabilitÃ© naturelle.
- **`faq_support` â†’ `temperature=0.3`, `max_tokens=1000`, `context_max_chunks=3`** : les FAQ attendent des rÃ©ponses courtes et ciblÃ©es. TempÃ©rature lÃ©gÃ¨rement plus haute pour un ton plus naturel. Peu de contexte nÃ©cessaire (les rÃ©ponses FAQ sont courtes).
- **`legal_compliance` â†’ `temperature=0.0`, `context_max_chunks=8`, `context_max_tokens=8000`, `footnote`** : tempÃ©rature Ã  zÃ©ro pour un dÃ©terminisme total (aucune place pour l'interprÃ©tation). Contexte maximal pour ne rien manquer. Citations en footnotes pour un format juridique.
- **`reports_analysis` â†’ `temperature=0.2`** : les rapports d'analyse nÃ©cessitent de la prÃ©cision mais aussi une capacitÃ© de synthÃ¨se, d'oÃ¹ une tempÃ©rature lÃ©gÃ¨rement plus haute que la doc technique.
- **`general` â†’ `temperature=0.7`** : profil conversationnel, ton plus naturel et engageant.
- **Tous `gpt-4o-mini`** : meilleur rapport qualitÃ©/prix pour la majoritÃ© des cas d'usage RAG. L'utilisateur peut changer pour un modÃ¨le plus puissant si nÃ©cessaire.

---

## 6. SpÃ©cifications techniques

### 6.1 SchÃ©ma Pydantic (backend)

```python
# ragkit/config/llm_schema.py
"""Pydantic schemas for LLM generation configuration."""

from __future__ import annotations

from enum import Enum

from pydantic import BaseModel, Field


class LLMProvider(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    OLLAMA = "ollama"
    MISTRAL = "mistral"


class CitationFormat(str, Enum):
    INLINE = "inline"
    FOOTNOTE = "footnote"


class ResponseLanguage(str, Enum):
    AUTO = "auto"
    FR = "fr"
    EN = "en"


DEFAULT_SYSTEM_PROMPT = """Tu es un assistant spÃ©cialisÃ© dans l'analyse de documents. Tu rÃ©ponds aux questions en te basant UNIQUEMENT sur le contexte fourni entre les balises <context> et </context>.

RÃ¨gles :
1. Base ta rÃ©ponse exclusivement sur le contexte fourni. Ne gÃ©nÃ¨re jamais d'information qui ne s'y trouve pas.
2. Cite tes sources en utilisant le format {citation_format_instruction} aprÃ¨s chaque affirmation importante.
3. Si l'information demandÃ©e n'est pas dans le contexte, dis-le honnÃªtement en utilisant la phrase : "{uncertainty_phrase}".
4. RÃ©ponds dans la langue de la question, sauf indication contraire.
5. Structure ta rÃ©ponse de maniÃ¨re claire avec des paragraphes, listes ou titres si nÃ©cessaire.
6. Si plusieurs sources se contredisent, signale-le explicitement."""


class LLMConfig(BaseModel):
    """LLM generation configuration."""

    # Provider & model
    provider: LLMProvider = LLMProvider.OPENAI
    model: str = "gpt-4o-mini"

    # Generation
    temperature: float = Field(default=0.1, ge=0.0, le=2.0)
    max_tokens: int = Field(default=2000, ge=100, le=16384)
    top_p: float = Field(default=0.9, ge=0.0, le=1.0)

    # Behavior
    cite_sources: bool = True
    citation_format: CitationFormat = CitationFormat.INLINE
    admit_uncertainty: bool = True
    uncertainty_phrase: str = "Je n'ai pas trouvÃ© cette information dans les documents disponibles."
    response_language: ResponseLanguage = ResponseLanguage.AUTO

    # Context
    context_max_chunks: int = Field(default=5, ge=1, le=30)
    context_max_tokens: int = Field(default=4000, ge=500, le=32000)

    # Prompt
    system_prompt: str = DEFAULT_SYSTEM_PROMPT

    # Advanced
    timeout: int = Field(default=60, ge=10, le=300)
    max_retries: int = Field(default=2, ge=0, le=5)
    streaming: bool = True
    debug_default: bool = False
```

### 6.2 Abstraction LLM Provider (backend)

```python
# ragkit/llm/base.py
"""Abstract base class for LLM providers."""

from __future__ import annotations

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import AsyncIterator


@dataclass
class LLMMessage:
    """A single message in the conversation."""
    role: str        # "system", "user", "assistant"
    content: str


@dataclass
class LLMUsage:
    """Token usage for a generation."""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


@dataclass
class LLMResponse:
    """Complete (non-streaming) response from the LLM."""
    content: str
    usage: LLMUsage
    model: str
    latency_ms: int


@dataclass
class LLMStreamChunk:
    """A single chunk from the streaming response."""
    content: str           # Partial text
    is_final: bool = False
    usage: LLMUsage | None = None  # Only on final chunk


@dataclass
class LLMTestResult:
    """Result from testing the LLM connection."""
    success: bool
    model: str
    response_text: str
    latency_ms: int
    error: str | None = None


class BaseLLMProvider(ABC):
    """Abstract base for LLM providers."""

    @abstractmethod
    async def generate(
        self,
        messages: list[LLMMessage],
        temperature: float = 0.1,
        max_tokens: int = 2000,
        top_p: float = 0.9,
    ) -> LLMResponse:
        """Generate a complete response."""
        ...

    @abstractmethod
    async def stream(
        self,
        messages: list[LLMMessage],
        temperature: float = 0.1,
        max_tokens: int = 2000,
        top_p: float = 0.9,
    ) -> AsyncIterator[LLMStreamChunk]:
        """Stream a response token by token."""
        ...

    @abstractmethod
    async def test_connection(self) -> LLMTestResult:
        """Test the LLM connection."""
        ...
```

### 6.3 Context Assembler (backend)

```python
# ragkit/llm/context_assembler.py
"""Assembles retrieved chunks into LLM context."""

from __future__ import annotations

from dataclasses import dataclass

import tiktoken

from ragkit.config.llm_schema import LLMConfig, CitationFormat


@dataclass
class ContextChunk:
    """A chunk prepared for LLM context injection."""
    source_id: int         # 1-indexed
    chunk_id: str
    text: str
    doc_title: str
    doc_path: str | None
    page_number: int | None
    score: float
    tokens: int


@dataclass
class AssembledContext:
    """The assembled context ready for prompt injection."""
    formatted_text: str
    chunks_used: list[ContextChunk]
    total_tokens: int
    chunks_available: int
    chunks_included: int
    truncated: bool


class ContextAssembler:
    """Selects and formats chunks for LLM context."""

    def __init__(self, config: LLMConfig):
        self.config = config
        try:
            self._encoder = tiktoken.encoding_for_model(config.model)
        except KeyError:
            self._encoder = tiktoken.get_encoding("cl100k_base")

    def assemble(
        self,
        results: list,          # SearchResult or RerankResult
    ) -> AssembledContext:
        """Assemble chunks into formatted context."""
        chunks: list[ContextChunk] = []
        total_tokens = 0
        truncated = False

        for i, result in enumerate(results):
            if len(chunks) >= self.config.context_max_chunks:
                break

            text = result.text
            tokens = len(self._encoder.encode(text))

            # Check token budget
            if total_tokens + tokens > self.config.context_max_tokens:
                # Truncate last chunk to fit budget
                remaining = self.config.context_max_tokens - total_tokens
                if remaining > 50:  # Minimum useful chunk
                    encoded = self._encoder.encode(text)[:remaining]
                    text = self._encoder.decode(encoded)
                    tokens = remaining
                    truncated = True
                else:
                    break

            chunks.append(ContextChunk(
                source_id=i + 1,
                chunk_id=result.chunk_id,
                text=text,
                doc_title=getattr(result, "doc_title", None) or "Document",
                doc_path=getattr(result, "doc_path", None),
                page_number=getattr(result, "page_number", None),
                score=result.score if hasattr(result, "score") else
                      result.rerank_score,
                tokens=tokens,
            ))
            total_tokens += tokens

        # Format as XML context
        formatted = self._format_context(chunks)

        return AssembledContext(
            formatted_text=formatted,
            chunks_used=chunks,
            total_tokens=total_tokens,
            chunks_available=len(results),
            chunks_included=len(chunks),
            truncated=truncated,
        )

    def _format_context(self, chunks: list[ContextChunk]) -> str:
        parts = ["<context>"]
        for chunk in chunks:
            page_attr = f' page="{chunk.page_number}"' if chunk.page_number else ""
            parts.append(
                f'<source id="{chunk.source_id}" '
                f'title="{chunk.doc_title}"{page_attr} '
                f'score="{chunk.score:.3f}">'
            )
            parts.append(chunk.text)
            parts.append("</source>")
        parts.append("</context>")
        return "\n".join(parts)
```

### 6.4 Response Generator (backend)

```python
# ragkit/llm/response_generator.py
"""Orchestrates context assembly + LLM generation."""

from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import AsyncIterator

from ragkit.config.llm_schema import LLMConfig, CitationFormat
from ragkit.llm.base import BaseLLMProvider, LLMMessage, LLMStreamChunk
from ragkit.llm.context_assembler import ContextAssembler, AssembledContext


@dataclass
class GenerationDebugInfo:
    """Debug info for the full generation pipeline."""
    model: str
    temperature: float
    retrieval_latency_ms: int
    context_chunks: int
    context_tokens: int
    context_truncated: bool
    prompt_tokens: int
    completion_tokens: int
    time_to_first_token_ms: int
    total_latency_ms: int
    estimated_cost_usd: float | None
    sources_used: list[dict]


@dataclass
class RAGResponse:
    """Complete RAG response with sources and debug."""
    content: str
    sources: list[dict]           # [{id, title, page, score, text_preview}]
    debug: GenerationDebugInfo | None = None


class ResponseGenerator:
    """Generates RAG responses using LLM + context."""

    def __init__(
        self,
        config: LLMConfig,
        llm_provider: BaseLLMProvider,
    ):
        self.config = config
        self.llm = llm_provider
        self.assembler = ContextAssembler(config)

    def _build_system_prompt(self) -> str:
        """Build the system prompt with variable substitution."""
        if self.config.citation_format == CitationFormat.INLINE:
            citation_instruction = "[Source: titre_document, p.N]"
        else:
            citation_instruction = "des notes de bas de page numÃ©rotÃ©es Â¹Â²Â³"

        return self.config.system_prompt.replace(
            "{citation_format_instruction}", citation_instruction
        ).replace(
            "{uncertainty_phrase}", self.config.uncertainty_phrase
        )

    async def generate(
        self,
        query: str,
        retrieval_results: list,
        retrieval_latency_ms: int = 0,
        include_debug: bool = False,
    ) -> RAGResponse:
        """Generate a complete (non-streaming) RAG response."""
        t_start = time.perf_counter()

        # 1. Assemble context
        context = self.assembler.assemble(retrieval_results)

        # 2. Build messages
        messages = [
            LLMMessage(role="system", content=self._build_system_prompt()),
            LLMMessage(
                role="user",
                content=f"{context.formatted_text}\n\nQuestion : {query}",
            ),
        ]

        # 3. Generate
        response = await self.llm.generate(
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            top_p=self.config.top_p,
        )

        t_total = time.perf_counter() - t_start

        # 4. Build sources list
        sources = self._build_sources(context)

        debug = None
        if include_debug:
            debug = GenerationDebugInfo(
                model=self.config.model,
                temperature=self.config.temperature,
                retrieval_latency_ms=retrieval_latency_ms,
                context_chunks=context.chunks_included,
                context_tokens=context.total_tokens,
                context_truncated=context.truncated,
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=response.usage.completion_tokens,
                time_to_first_token_ms=response.latency_ms,
                total_latency_ms=int(t_total * 1000),
                estimated_cost_usd=self._estimate_cost(response.usage),
                sources_used=[
                    {"id": c.source_id, "title": c.doc_title,
                     "page": c.page_number, "score": c.score}
                    for c in context.chunks_used
                ],
            )

        return RAGResponse(
            content=response.content,
            sources=sources,
            debug=debug,
        )

    async def stream(
        self,
        query: str,
        retrieval_results: list,
        include_debug: bool = False,
    ) -> AsyncIterator[LLMStreamChunk]:
        """Stream a RAG response token by token."""
        context = self.assembler.assemble(retrieval_results)

        messages = [
            LLMMessage(role="system", content=self._build_system_prompt()),
            LLMMessage(
                role="user",
                content=f"{context.formatted_text}\n\nQuestion : {query}",
            ),
        ]

        async for chunk in self.llm.stream(
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            top_p=self.config.top_p,
        ):
            yield chunk

    def _build_sources(self, context: AssembledContext) -> list[dict]:
        return [
            {
                "id": c.source_id,
                "chunk_id": c.chunk_id,
                "title": c.doc_title,
                "path": c.doc_path,
                "page": c.page_number,
                "score": c.score,
                "text_preview": c.text[:200] + "..." if len(c.text) > 200 else c.text,
            }
            for c in context.chunks_used
        ]

    def _estimate_cost(self, usage) -> float | None:
        # Simplified cost estimation for known models
        costs = {
            "gpt-4o-mini": (0.15, 0.60),
            "gpt-4o": (2.50, 10.00),
            "gpt-4-turbo": (10.0, 30.0),
            "claude-3-5-sonnet-20241022": (3.0, 15.0),
            "claude-3-haiku-20240307": (0.25, 1.25),
            "mistral-large-latest": (2.0, 6.0),
            "mistral-small-latest": (0.2, 0.6),
        }
        rate = costs.get(self.config.model)
        if not rate:
            return None
        input_cost = usage.prompt_tokens / 1_000_000 * rate[0]
        output_cost = usage.completion_tokens / 1_000_000 * rate[1]
        return round(input_cost + output_cost, 6)
```

### 6.5 API REST (routes backend)

#### 6.5.1 Routes Config LLM

| Endpoint | MÃ©thode | Description | Corps | RÃ©ponse |
|----------|---------|-------------|-------|---------|
| `/api/llm/config` | GET | Config LLM courante | â€” | `LLMConfig` |
| `/api/llm/config` | PUT | Met Ã  jour la config | `LLMConfig` (partiel) | `LLMConfig` |
| `/api/llm/config/reset` | POST | RÃ©initialise au profil actif | â€” | `LLMConfig` |

#### 6.5.2 Routes Actions LLM

| Endpoint | MÃ©thode | Description | Corps | RÃ©ponse |
|----------|---------|-------------|-------|---------|
| `/api/llm/test-connection` | POST | Test de connexion au LLM | â€” | `LLMTestResult` |
| `/api/llm/models` | GET | Liste des modÃ¨les par provider | `?provider=openai` | `LLMModelInfo[]` |

#### 6.5.3 Route Chat (pipeline complet)

| Endpoint | MÃ©thode | Description | Corps | RÃ©ponse |
|----------|---------|-------------|-------|---------|
| `/api/chat` | POST | Pipeline complet : recherche â†’ reranking â†’ gÃ©nÃ©ration | `ChatQuery` | `ChatResponse` (ou SSE stream) |
| `/api/chat/stream` | POST | MÃªme pipeline mais en streaming (SSE) | `ChatQuery` | `text/event-stream` |

#### 6.5.4 ModÃ¨les de requÃªte et rÃ©ponse

```python
class ChatQuery(BaseModel):
    """Complete chat query â€” triggers the full RAG pipeline."""
    query: str = Field(..., min_length=1, max_length=5000)
    search_type: SearchType | None = None
    alpha: float | None = None
    filters: SearchFilters | None = None
    include_debug: bool = False

class ChatResponse(BaseModel):
    """Complete chat response with generated answer and sources."""
    query: str
    answer: str                         # Markdown text generated by LLM
    sources: list[ChatSource]
    search_type: str
    debug: ChatDebugInfo | None = None

class ChatSource(BaseModel):
    id: int
    chunk_id: str
    title: str
    path: str | None
    page: int | None
    score: float
    text_preview: str

class ChatDebugInfo(BaseModel):
    # Retrieval
    retrieval_latency_ms: int
    search_type: str
    chunks_retrieved: int
    reranking_applied: bool
    # Context
    context_chunks: int
    context_tokens: int
    context_truncated: bool
    # Generation
    model: str
    temperature: float
    prompt_tokens: int
    completion_tokens: int
    time_to_first_token_ms: int
    total_latency_ms: int
    estimated_cost_usd: float | None
    # Sources detail
    sources_detail: list[dict]

class LLMModelInfo(BaseModel):
    id: str
    name: str
    provider: str
    context_window: int
    cost_input: str | None
    cost_output: str | None
    languages: str
    quality_rating: int
```

### 6.6 Streaming via Tauri

Le streaming nÃ©cessite un mÃ©canisme spÃ©cifique dans Tauri. L'approche retenue est l'**Event Emitter** :

```rust
// desktop/src-tauri/src/commands.rs (ajouts Ã‰tape 9)

#[tauri::command]
pub async fn chat_stream(
    window: tauri::Window,
    query: serde_json::Value,
) -> Result<String, String> {
    // 1. Start retrieval pipeline
    // 2. For each LLM token received:
    window.emit("chat-stream-chunk", payload)?;
    // 3. On completion:
    window.emit("chat-stream-done", final_payload)?;
    Ok("stream_started".to_string())
}

#[tauri::command]
pub async fn chat_stream_stop() -> Result<(), String> {
    // Cancel the ongoing generation
    Ok(())
}
```

CÃ´tÃ© React :

```typescript
// hooks/useChatStream.ts
export function useChatStream() {
  const [content, setContent] = useState("");
  const [isStreaming, setIsStreaming] = useState(false);

  const startStream = async (query: ChatQuery) => {
    setIsStreaming(true);
    setContent("");

    const unlisten = await listen<string>("chat-stream-chunk", (event) => {
      setContent((prev) => prev + event.payload);
    });

    await listen("chat-stream-done", (event) => {
      setIsStreaming(false);
      unlisten();
      // event.payload contains sources and debug info
    });

    await invoke("chat_stream", { query });
  };

  const stopStream = () => invoke("chat_stream_stop");

  return { content, isStreaming, startStream, stopStream };
}
```

### 6.7 Commandes Tauri (Rust) â€” ajouts

```rust
// LLM config
#[tauri::command]
pub async fn get_llm_config() -> Result<serde_json::Value, String> { ... }
#[tauri::command]
pub async fn update_llm_config(config: serde_json::Value) -> Result<serde_json::Value, String> { ... }
#[tauri::command]
pub async fn reset_llm_config() -> Result<serde_json::Value, String> { ... }

// LLM actions
#[tauri::command]
pub async fn test_llm_connection() -> Result<serde_json::Value, String> { ... }
#[tauri::command]
pub async fn get_llm_models(provider: String) -> Result<serde_json::Value, String> { ... }

// Chat (full pipeline)
#[tauri::command]
pub async fn chat(query: serde_json::Value) -> Result<serde_json::Value, String> { ... }
#[tauri::command]
pub async fn chat_stream(window: tauri::Window, query: serde_json::Value) -> Result<String, String> { ... }
#[tauri::command]
pub async fn chat_stream_stop() -> Result<(), String> { ... }
```

### 6.8 Composants React â€” arborescence

```
desktop/src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ settings/
â”‚   â”‚   â”œâ”€â”€ LLMSettings.tsx                    â† NOUVEAU : section complÃ¨te
â”‚   â”‚   â”œâ”€â”€ LLMProviderSelector.tsx            â† NOUVEAU : cartes provider
â”‚   â”‚   â”œâ”€â”€ LLMModelSelector.tsx               â† NOUVEAU : dropdown + fiche
â”‚   â”‚   â”œâ”€â”€ GenerationParams.tsx               â† NOUVEAU : temperature, max_tokens, top_p
â”‚   â”‚   â”œâ”€â”€ BehaviorPanel.tsx                  â† NOUVEAU : citations, incertitude, langue
â”‚   â”‚   â”œâ”€â”€ ContextPanel.tsx                   â† NOUVEAU : max_chunks, max_tokens
â”‚   â”‚   â”œâ”€â”€ SystemPromptEditor.tsx             â† NOUVEAU : Ã©diteur de prompt
â”‚   â”‚   â”œâ”€â”€ GeneralSettings.tsx                â† MODIFIER : ajouter LLM params
â”‚   â”‚   â””â”€â”€ ... (existants)
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ ChatView.tsx                       â† REFACTORING MAJEUR
â”‚   â”‚   â”œâ”€â”€ ChatMessage.tsx                    â† NOUVEAU : bulle message (user/assistant)
â”‚   â”‚   â”œâ”€â”€ AssistantMessage.tsx               â† NOUVEAU : rÃ©ponse MD + sources
â”‚   â”‚   â”œâ”€â”€ MarkdownRenderer.tsx               â† NOUVEAU : rendu markdown
â”‚   â”‚   â”œâ”€â”€ SourcesPanel.tsx                   â† NOUVEAU : panneau sources
â”‚   â”‚   â”œâ”€â”€ SourceCard.tsx                     â† NOUVEAU : carte source cliquable
â”‚   â”‚   â”œâ”€â”€ CitationLink.tsx                   â† NOUVEAU : lien citation inline
â”‚   â”‚   â”œâ”€â”€ StreamingIndicator.tsx             â† NOUVEAU : animation "GÃ©nÃ©ration..."
â”‚   â”‚   â”œâ”€â”€ StopButton.tsx                     â† NOUVEAU : bouton arrÃªt streaming
â”‚   â”‚   â”œâ”€â”€ GenerationDebugPanel.tsx           â† NOUVEAU : debug complet pipeline
â”‚   â”‚   â””â”€â”€ ... (existants)
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ ... (existants)
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useLLMConfig.ts                        â† NOUVEAU : hook config
â”‚   â”œâ”€â”€ useLLMTest.ts                          â† NOUVEAU : hook test connexion
â”‚   â”œâ”€â”€ useChat.ts                             â† NOUVEAU : hook pipeline complet
â”‚   â”œâ”€â”€ useChatStream.ts                       â† NOUVEAU : hook streaming
â”‚   â””â”€â”€ ... (existants)
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ ipc.ts                                 â† MODIFIER : ajouter routes LLM + chat
â””â”€â”€ locales/
    â”œâ”€â”€ fr.json                                â† MODIFIER : ajouter clÃ©s LLM
    â””â”€â”€ en.json                                â† MODIFIER : ajouter clÃ©s LLM
```

### 6.9 Persistance

```json
{
  "general": {
    "ingestion_mode": "manual",
    "search_type": "hybrid",
    "llm_model": "openai/gpt-4o-mini",
    "llm_temperature": 0.1,
    "response_language": "auto"
  },
  "llm": {
    "provider": "openai",
    "model": "gpt-4o-mini",
    "temperature": 0.1,
    "max_tokens": 2000,
    "top_p": 0.9,
    "cite_sources": true,
    "citation_format": "inline",
    "admit_uncertainty": true,
    "uncertainty_phrase": "Je n'ai pas trouvÃ© cette information dans les documents disponibles.",
    "response_language": "auto",
    "context_max_chunks": 5,
    "context_max_tokens": 4000,
    "system_prompt": "...",
    "timeout": 60,
    "max_retries": 2,
    "streaming": true,
    "debug_default": false
  }
}
```

Les clÃ©s API (OpenAI, Anthropic, Mistral) sont stockÃ©es via le systÃ¨me de secrets (Ã‰tape 3).

### 6.10 DÃ©pendances Python ajoutÃ©es

```toml
# pyproject.toml â€” ajouts pour Ã‰tape 9
dependencies = [
    # ... (existants Ã‰tapes 0-8)
    "openai>=1.12",               # Client OpenAI (chat completions + streaming)
    "anthropic>=0.18",            # Client Anthropic (messages + streaming)
    "tiktoken>=0.5",              # Comptage de tokens pour le budget contexte
    # httpx est dÃ©jÃ  prÃ©sent (Ã‰tape 8) â€” utilisÃ© pour Mistral et Ollama
]
```

---

## 7. CritÃ¨res d'acceptation

### 7.1 Fonctionnels

| # | CritÃ¨re |
|---|---------|
| F1 | La section `PARAMÃˆTRES > ParamÃ¨tres avancÃ©s > LLM / GÃ‰NÃ‰RATION` est accessible et affiche tous les paramÃ¨tres |
| F2 | Le sÃ©lecteur de provider propose OpenAI, Anthropic, Ollama, Mistral |
| F3 | Le provider Ollama est grisÃ© si Ollama n'est pas dÃ©tectÃ© |
| F4 | La fiche modÃ¨le affiche contexte, langues, coÃ»t et latence estimÃ©e |
| F5 | Le champ clÃ© API est visible pour les providers cloud et fonctionne (masquage, test) |
| F6 | Le bouton "Tester la connexion" valide la connexion au LLM |
| F7 | Les sliders (tempÃ©rature, max_tokens, top_p) sont fonctionnels avec validation des bornes |
| F8 | Les toggles de comportement (cite_sources, admit_uncertainty) fonctionnent |
| F9 | Le sÃ©lecteur de format de citation (inline / footnote) modifie les instructions du prompt |
| F10 | L'Ã©diteur de prompt systÃ¨me est fonctionnel avec compteur de tokens |
| F11 | Le bouton "Restaurer le prompt par dÃ©faut" fonctionne |
| F12 | Les ParamÃ¨tres gÃ©nÃ©raux affichent les 3 nouveaux champs (modÃ¨le, tempÃ©rature, langue) |
| F13 | Modifier un paramÃ¨tre dans ParamÃ¨tres gÃ©nÃ©raux le synchronise dans ParamÃ¨tres avancÃ©s (et inversement) |
| F14 | Le CHAT affiche les rÃ©ponses du LLM en markdown avec mise en forme |
| F15 | Le streaming affiche les tokens progressivement |
| F16 | L'indicateur passe de "Recherche en cours..." Ã  "GÃ©nÃ©ration en cours..." |
| F17 | Le bouton "ArrÃªter" interrompt la gÃ©nÃ©ration streaming |
| F18 | Les citations `[Source: nom, p.N]` sont rendues dans la rÃ©ponse |
| F19 | Le panneau Sources s'affiche sous chaque rÃ©ponse avec les chunks utilisÃ©s |
| F20 | Cliquer sur une source dÃ©veloppe l'extrait du chunk |
| F21 | Quand le LLM ne trouve pas l'info, il affiche la phrase d'incertitude |
| F22 | Le mode debug affiche le pipeline complet (retrieval + context + gÃ©nÃ©ration + coÃ»t) |
| F23 | Le badge "ModifiÃ©" apparaÃ®t Ã  cÃ´tÃ© de chaque paramÃ¨tre modifiÃ© |
| F24 | Le bouton "RÃ©initialiser au profil" restaure les valeurs par dÃ©faut |
| F25 | Tous les textes sont traduits FR/EN via i18n |

### 7.2 Techniques

| # | CritÃ¨re |
|---|---------|
| T1 | `GET /api/llm/config` retourne la config courante |
| T2 | `PUT /api/llm/config` valide et persiste les modifications |
| T3 | `POST /api/llm/config/reset` restaure les valeurs du profil actif |
| T4 | `POST /api/llm/test-connection` teste la connexion au LLM configurÃ© |
| T5 | Le provider OpenAI fonctionne avec `gpt-4o-mini` (gÃ©nÃ©ration + streaming) |
| T6 | Le provider Anthropic fonctionne avec `claude-3-5-sonnet` (gÃ©nÃ©ration + streaming) |
| T7 | Le provider Ollama fonctionne avec un modÃ¨le local installÃ© (gÃ©nÃ©ration + streaming) |
| T8 | Le provider Mistral fonctionne avec `mistral-small-latest` (gÃ©nÃ©ration + streaming) |
| T9 | Le Context Assembler respecte `context_max_chunks` et `context_max_tokens` |
| T10 | Le Context Assembler tronque correctement le dernier chunk si le budget est dÃ©passÃ© |
| T11 | Le prompt systÃ¨me inclut les variables substituÃ©es (`{citation_format_instruction}`, `{uncertainty_phrase}`) |
| T12 | `POST /api/chat` exÃ©cute le pipeline complet : recherche â†’ reranking â†’ assemblage â†’ gÃ©nÃ©ration |
| T13 | `POST /api/chat/stream` retourne un flux SSE avec les tokens progressifs |
| T14 | Le streaming via Tauri events fonctionne (Ã©mission + rÃ©ception cÃ´tÃ© React) |
| T15 | L'arrÃªt du streaming interrompt effectivement la gÃ©nÃ©ration |
| T16 | L'estimation de coÃ»t est correcte pour les modÃ¨les connus |
| T17 | Les clÃ©s API sont stockÃ©es dans le keyring (pas dans `settings.json`) |
| T18 | Le timeout et les retries fonctionnent pour chaque provider |
| T19 | La config LLM est persistÃ©e dans `settings.json` sous `llm` |
| T20 | Les paramÃ¨tres gÃ©nÃ©raux sont synchronisÃ©s avec `llm.*` |
| T21 | `tsc --noEmit` ne produit aucune erreur TypeScript |
| T22 | Le CI passe sur les 4 targets (lint + build) |

---

## 8. PÃ©rimÃ¨tre exclus (Ã‰tape 9)

- **Historique de conversation / MÃ©moire** : sera ajoutÃ© Ã  l'Ã‰tape 10 (Agents). Chaque requÃªte est indÃ©pendante Ã  cette Ã©tape.
- **Query Analyzer / Intent Detection** : sera ajoutÃ© Ã  l'Ã‰tape 10. Toute requÃªte passe par le pipeline RAG (mÃªme "Bonjour").
- **Query Rewriting** : sera ajoutÃ© Ã  l'Ã‰tape 10.
- **ModÃ¨le de fallback** (second modÃ¨le en cas de panne du premier) : amÃ©lioration future.
- **Few-shot examples** dans le prompt : amÃ©lioration future.
- **Chain-of-thought prompting** : amÃ©lioration future.
- **Context compression** (rÃ©sumÃ© des chunks avant injection) : amÃ©lioration future.
- **Streaming des sources** (afficher les sources avant la fin de la gÃ©nÃ©ration) : amÃ©lioration future.
- **Ã‰valuation automatique** de la qualitÃ© des rÃ©ponses : sera ajoutÃ©e Ã  l'Ã‰tape 11 (Monitoring).
- **ModÃ¨les custom / fine-tunÃ©s** : non pertinent pour la V1.
- **Prompt par type de requÃªte** (prompt diffÃ©rent pour les rÃ©sumÃ©s, analyses, comparaisons) : amÃ©lioration future.

---

## 9. Estimation

| TÃ¢che | Effort estimÃ© |
|-------|---------------|
| SchÃ©ma Pydantic `LLMConfig` + validation | 0.5 jour |
| Abstraction `BaseLLMProvider` + dataclasses | 0.5 jour |
| Provider OpenAI (gÃ©nÃ©ration + streaming + test) | 1.5 jours |
| Provider Anthropic (gÃ©nÃ©ration + streaming + test) | 1.5 jours |
| Provider Ollama (gÃ©nÃ©ration + streaming + test + dÃ©tection) | 1.5 jours |
| Provider Mistral (gÃ©nÃ©ration + streaming + test) | 1 jour |
| Factory `create_llm_provider` | 0.5 jour |
| `ContextAssembler` (sÃ©lection, budget tokens, formatage XML) | 1.5 jours |
| `ResponseGenerator` (orchestration, prompt, debug, coÃ»t) | 1.5 jours |
| Routes API config LLM (CRUD) | 0.5 jour |
| Routes API actions (test-connection, models) | 0.5 jour |
| Routes API chat + chat/stream (pipeline complet) | 1.5 jours |
| Commandes Tauri (config + chat + streaming events) | 1 jour |
| Composant `LLMSettings.tsx` (section paramÃ¨tres complÃ¨te) | 1 jour |
| Composants `LLMProviderSelector.tsx`, `LLMModelSelector.tsx` | 1 jour |
| Composants `GenerationParams.tsx`, `BehaviorPanel.tsx`, `ContextPanel.tsx` | 1 jour |
| Composant `SystemPromptEditor.tsx` (Ã©diteur + compteur tokens + reset) | 0.5 jour |
| Modification `GeneralSettings.tsx` (3 nouveaux champs synchronisÃ©s) | 0.5 jour |
| Refactoring `ChatView.tsx` (passage de rÃ©sultats bruts Ã  conversation) | 1.5 jours |
| Composants `ChatMessage.tsx`, `AssistantMessage.tsx`, `MarkdownRenderer.tsx` | 1.5 jours |
| Composants `SourcesPanel.tsx`, `SourceCard.tsx`, `CitationLink.tsx` | 1 jour |
| Composants `StreamingIndicator.tsx`, `StopButton.tsx` | 0.5 jour |
| Composant `GenerationDebugPanel.tsx` | 0.5 jour |
| Hooks (`useLLMConfig`, `useLLMTest`, `useChat`, `useChatStream`) | 1 jour |
| Traductions i18n (FR + EN) | 0.5 jour |
| Tests unitaires providers (OpenAI, Anthropic, Ollama, Mistral â€” mock API) | 2 jours |
| Tests unitaires `ContextAssembler` (sÃ©lection, budget, tronquage) | 0.5 jour |
| Tests unitaires `ResponseGenerator` (prompt, variables, debug) | 0.5 jour |
| Tests d'intÃ©gration (pipeline complet : query â†’ retrieval â†’ reranking â†’ gÃ©nÃ©ration) | 1.5 jours |
| Tests manuels + corrections | 1.5 jours |
| **Total** | **~29 jours** |
