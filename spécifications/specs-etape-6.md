# ğŸ§° RAGKIT Desktop â€” SpÃ©cifications Ã‰tape 6 : Recherche lexicale (BM25)

> **Ã‰tape** : 6 â€” Recherche lexicale (BM25)  
> **Tag cible** : `v0.7.0`  
> **Date** : 17 fÃ©vrier 2026  
> **DÃ©pÃ´t** : https://github.com/henribesnard/ragkit_desktop.git  
> **PrÃ©requis** : Ã‰tape 5 (Recherche sÃ©mantique) implÃ©mentÃ©e et validÃ©e

---

## 1. Objectif

Ajouter un **second mode de recherche** basÃ© sur la **correspondance lexicale (BM25)**, complÃ©mentaire Ã  la recherche sÃ©mantique. La recherche lexicale excelle pour les cas oÃ¹ les **termes exacts** comptent : codes produit, rÃ©fÃ©rences, noms propres, acronymes, identifiants techniques.

Cette Ã©tape livre :
- Une section `PARAMÃˆTRES > ParamÃ¨tres avancÃ©s > RECHERCHE LEXICALE` complÃ¨te et fonctionnelle.
- Un **moteur de recherche BM25** avec index inversÃ© persistant, tokenization configurable, gestion des stopwords multilingues et stemming.
- La **construction automatique de l'index BM25** lors de l'ingestion (le pipeline d'ingestion est Ã©tendu pour alimenter l'index lexical en parallÃ¨le du stockage vectoriel).
- Un **sÃ©lecteur de mode de recherche** dans le CHAT permettant de basculer entre recherche sÃ©mantique et recherche lexicale.
- La **mise en Ã©vidence des termes matchÃ©s** dans les rÃ©sultats de la recherche lexicale.
- Le support des algorithmes **BM25 classique** et **BM25+** (variante avec bonus pour les termes prÃ©sents).

**Pas de fusion hybride** Ã  cette Ã©tape. L'utilisateur choisit manuellement entre sÃ©mantique ou lexical. La fusion sera ajoutÃ©e Ã  l'Ã‰tape 7.

---

## 2. SpÃ©cifications fonctionnelles

### 2.1 Section PARAMÃˆTRES > ParamÃ¨tres avancÃ©s > RECHERCHE LEXICALE

#### Structure de l'onglet PARAMÃˆTRES Ã  cette Ã©tape

```
PARAMÃˆTRES
â”œâ”€â”€ ParamÃ¨tres gÃ©nÃ©raux
â”‚   â””â”€â”€ Mode d'ingestion (Manuel / Automatique)     â† Ã‰tape 4
â””â”€â”€ ParamÃ¨tres avancÃ©s
    â”œâ”€â”€ INGESTION & PRÃ‰PROCESSING                    â† Ã‰tape 1
    â”œâ”€â”€ CHUNKING                                     â† Ã‰tape 2
    â”œâ”€â”€ EMBEDDING                                    â† Ã‰tape 3
    â”œâ”€â”€ BASE DE DONNÃ‰ES VECTORIELLE                  â† Ã‰tape 4
    â”œâ”€â”€ RECHERCHE SÃ‰MANTIQUE                         â† Ã‰tape 5
    â””â”€â”€ RECHERCHE LEXICALE                           â† NOUVEAU
```

#### Layout de la section RECHERCHE LEXICALE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RECHERCHE LEXICALE                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ ParamÃ¨tres principaux â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â˜‘ Recherche lexicale activÃ©e                             â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  Algorithme :               (â€¢) BM25       â—‹ BM25+        â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  Nombre de rÃ©sultats (top_k) :  [===â—†======] 15           â”‚ â”‚
â”‚  â”‚  Poids (recherche hybride) :    [====â—†=====] 0.5          â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â„¹ï¸ BM25 est l'algorithme standard de recherche lexicale.  â”‚ â”‚
â”‚  â”‚  BM25+ ajoute un bonus pour les termes prÃ©sents, ce qui   â”‚ â”‚
â”‚  â”‚  amÃ©liore le classement pour les documents courts.         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ ParamÃ¨tres BM25 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  k1 (saturation du terme) : [====â—†=====] 1.5              â”‚ â”‚
â”‚  â”‚  b  (normalisation de longueur) : [=====â—†====] 0.75       â”‚ â”‚
â”‚  â”‚  delta (BM25+ uniquement) : [====â—†=====] 0.5              â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â„¹ï¸ k1 Ã©levÃ© = la rÃ©pÃ©tition d'un terme compte davantage.  â”‚ â”‚
â”‚  â”‚  b Ã©levÃ© = les documents courts sont favorisÃ©s.            â”‚ â”‚
â”‚  â”‚  Valeurs recommandÃ©es : k1 = 1.2â€“2.0, b = 0.5â€“0.8.       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ Preprocessing lexical â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â˜‘ Conversion en minuscules (lowercase)                   â”‚ â”‚
â”‚  â”‚  â˜‘ Suppression des stopwords                              â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  Langue des stopwords :  [â–¾ auto (dÃ©tection)       ]      â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â˜‘ Stemming (racinisation)                                â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  Langue du stemmer :     [â–¾ auto (dÃ©tection)       ]      â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â„¹ï¸ Le stemming rÃ©duit les mots Ã  leur racine :            â”‚ â”‚
â”‚  â”‚  "courant", "courir", "coureur" â†’ "cour".                 â”‚ â”‚
â”‚  â”‚  AmÃ©liore le rappel mais peut crÃ©er de faux positifs.     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ Ã‰tat de l'index BM25 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  ğŸ“Š Index : 1 247 documents Â· 42 318 termes uniques       â”‚ â”‚
â”‚  â”‚  ğŸ’¾ Taille : 8.2 Mo Â· DerniÃ¨re mise Ã  jour : v3 (15 fÃ©v) â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  [ğŸ”„ Reconstruire l'index]                                 â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  â„¹ï¸ L'index BM25 est construit automatiquement lors de     â”‚ â”‚
â”‚  â”‚  l'ingestion. Le reconstruire manuellement est utile       â”‚ â”‚
â”‚  â”‚  aprÃ¨s un changement de paramÃ¨tres de preprocessing.       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â–¸ ParamÃ¨tres avancÃ©s                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Seuil de score minimum :    [â—†=========] 0.0             â”‚ â”‚
â”‚  â”‚  N-grams :                   [â–¾ Unigrams (1,1)     ]      â”‚ â”‚
â”‚  â”‚  â˜ Mode debug activÃ© par dÃ©faut                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  [â†» RÃ©initialiser au profil]                                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 SÃ©lecteur de mode de recherche dans le CHAT

L'Ã‰tape 6 ajoute un **sÃ©lecteur de mode de recherche** dans la barre de recherche du CHAT :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ’¬ CHAT                                           [âš™ Options] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ Barre de recherche â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  [â–¾ ğŸ” SÃ©mantique â–¾]                                      â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚     ğŸ” SÃ©mantique â€” Recherche par sens et concepts         â”‚ â”‚
â”‚  â”‚     ğŸ“ Lexicale â€” Recherche par mots-clÃ©s exacts           â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  [Posez votre question...                           ] [â†’]  â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  Filtres rapides :                                        â”‚ â”‚
â”‚  â”‚  [â–¾ Tous les documents] [â–¾ Toutes les langues]           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”€â”€ RÃ©sultats pour "article 12 rÃ©siliation" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â”€â”€ Mode : Lexicale (BM25) Â· 8 rÃ©sultats Â· 32 ms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ RÃ©sultat #1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Score BM25 : 18.42 â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  ğŸ“„ contrat-service-2024.pdf Â· Page 8                     â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  "Les conditions de ã€rÃ©siliationã€‘ anticipÃ©e sont          â”‚ â”‚
â”‚  â”‚  dÃ©finies Ã  l'ã€articleã€‘ã€12ã€‘ du prÃ©sent contrat. Le       â”‚ â”‚
â”‚  â”‚  prestataire peut rÃ©silier le contrat avec un prÃ©avis..."  â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚  ğŸ“ Juridique Â· ğŸ· contrat, rÃ©siliation Â· ğŸŒ fr          â”‚ â”‚
â”‚  â”‚  Termes matchÃ©s : article (Ã—2), 12 (Ã—1), rÃ©siliation (Ã—3) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€ RÃ©sultat #2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Score BM25 : 14.87 â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ...                                                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Comportements du sÃ©lecteur** :
- Le sÃ©lecteur est un dropdown au-dessus de la barre de recherche.
- Deux options disponibles Ã  cette Ã©tape : **SÃ©mantique** (ğŸ”) et **Lexicale** (ğŸ“).
- Le mode par dÃ©faut est SÃ©mantique (le mode hybride sera ajoutÃ© Ã  l'Ã‰tape 7).
- Le mode sÃ©lectionnÃ© est mÃ©morisÃ© pour la session mais pas persistÃ© (il revient Ã  "SÃ©mantique" au redÃ©marrage).
- Un tooltip dÃ©crit chaque mode au survol.

### 2.3 DiffÃ©rences d'affichage : lexical vs sÃ©mantique

| Ã‰lÃ©ment | Recherche sÃ©mantique (Ã‰t. 5) | Recherche lexicale (Ã‰t. 6) |
|---------|------------------------------|---------------------------|
| **Label du score** | "Score : 0.892" (similaritÃ© 0â€“1) | "Score BM25 : 18.42" (non bornÃ©) |
| **Code couleur du score** | Vertâ†’Rouge basÃ© sur 0â€“1 | Relatif au score max de la requÃªte (le meilleur rÃ©sultat est toujours vert) |
| **Surlignage** | RequÃªte surlignÃ©e (fond jaune) | Termes matchÃ©s surlignÃ©s avec ã€crochets grasã€‘ |
| **Info supplÃ©mentaire** | â€” | "Termes matchÃ©s : article (Ã—2), rÃ©siliation (Ã—3)" |
| **En-tÃªte de rÃ©sultats** | "Mode : SÃ©mantique" | "Mode : Lexicale (BM25)" |
| **Latence debug** | Embedding + recherche + MMR | Tokenization + recherche BM25 |

### 2.4 Mise en Ã©vidence des termes matchÃ©s

Pour la recherche lexicale, les termes de la requÃªte qui apparaissent dans le texte du chunk sont mis en Ã©vidence.

**RÃ¨gles de surlignage** :
1. Les termes sont identifiÃ©s aprÃ¨s tokenization (mÃªme pipeline que la requÃªte : lowercase, stemming).
2. Un terme matchÃ© dans le texte original est encadrÃ© par une balise de surlignage (`<mark>`).
3. Si le stemming est activÃ©, les variantes morphologiques sont aussi surlignÃ©es : recherche de "rÃ©siliation" surligne aussi "rÃ©silier", "rÃ©siliÃ©e", "rÃ©siliÃ©".
4. Les stopwords ne sont jamais surlignÃ©s mÃªme s'ils apparaissent dans la requÃªte.
5. Le compteur "Termes matchÃ©s" affiche chaque terme de la requÃªte (aprÃ¨s stopwords removal) avec le nombre d'occurrences dans le chunk.

### 2.5 Construction de l'index BM25 lors de l'ingestion

L'index BM25 est construit **automatiquement** lors de chaque ingestion (Ã‰tape 4). Le pipeline d'ingestion est Ã©tendu :

```
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â”Œâ”€â”€â†’â”‚  STOCKAGE   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚ VECTORIEL   â”‚
â”‚ PARSING  â”‚â†’ â”‚ CHUNKING â”‚â†’ â”‚EMBEDDING â”‚â”€â”€â”€â”€â”€â”€â”¤   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ (Ã‰t. 1)  â”‚  â”‚ (Ã‰t. 2)  â”‚  â”‚ (Ã‰t. 3)  â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                          â””â”€â”€â†’â”‚ INDEX BM25  â”‚
                    â”‚                              â”‚ (Ã‰t. 6)     â”‚
                    â””â”€ texte des chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Points clÃ©s** :
- L'index BM25 est alimentÃ© par les **textes des chunks** (pas les vecteurs).
- Il est construit en parallÃ¨le du stockage vectoriel (non sÃ©quentiel).
- Il partage les mÃªmes `chunk_id` que le stockage vectoriel pour pouvoir rÃ©cupÃ©rer les payloads.
- L'index est **persistÃ© sur disque** dans `~/.ragkit/data/bm25_index/`.
- L'index est reconstruit lors d'une ingestion complÃ¨te et mis Ã  jour de maniÃ¨re incrÃ©mentale (ajout/suppression de documents).

### 2.6 Panneau "Ã‰tat de l'index BM25"

Ce panneau affiche des statistiques sur l'index lexical :

| MÃ©trique | Source | Description |
|----------|--------|-------------|
| Documents | Nombre de chunks indexÃ©s | CohÃ©rent avec le nombre de vecteurs |
| Termes uniques | Taille du vocabulaire | AprÃ¨s tokenization + stopwords + stemming |
| Taille | Taille du fichier d'index sur disque | En Mo |
| DerniÃ¨re mise Ã  jour | Version d'ingestion associÃ©e | RÃ©fÃ©rence `ingestion_history` |

**Action "Reconstruire l'index"** : recalcule l'index BM25 Ã  partir des textes des chunks dÃ©jÃ  stockÃ©s (sans relancer le pipeline complet). Utile aprÃ¨s un changement de paramÃ¨tres de preprocessing lexical (lowercase, stopwords, stemming) qui ne nÃ©cessite pas de rÃ©ingestion complÃ¨te.

---

## 3. Algorithme BM25

### 3.1 Formule BM25 classique

```
BM25(D, Q) = Î£ IDF(qi) Ã— ( f(qi, D) Ã— (k1 + 1) )
              i            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                           f(qi, D) + k1 Ã— (1 - b + b Ã— |D| / avgdl)

oÃ¹ :
  Q         = requÃªte (ensemble de termes q1, q2, ..., qn)
  D         = document (chunk)
  f(qi, D)  = frÃ©quence du terme qi dans le document D
  |D|       = longueur du document D (en tokens)
  avgdl     = longueur moyenne des documents dans le corpus
  k1        = paramÃ¨tre de saturation du terme (dÃ©faut : 1.5)
  b         = paramÃ¨tre de normalisation de longueur (dÃ©faut : 0.75)

  IDF(qi)   = ln( (N - n(qi) + 0.5) / (n(qi) + 0.5) + 1 )
  N         = nombre total de documents dans le corpus
  n(qi)     = nombre de documents contenant le terme qi
```

### 3.2 Variante BM25+

BM25+ ajoute un bonus `delta` pour les termes prÃ©sents, ce qui corrige un biais de BM25 classique qui peut attribuer un score nÃ©gatif aux termes rares dans les documents longs.

```
BM25+(D, Q) = Î£ IDF(qi) Ã— ( f(qi, D) Ã— (k1 + 1)
               i            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ + delta )
                             f(qi, D) + k1 Ã— (1 - b + b Ã— |D| / avgdl)

  delta     = bonus pour les termes prÃ©sents (dÃ©faut : 0.5)
```

### 3.3 Pipeline de tokenization

La mÃªme pipeline de tokenization est appliquÃ©e aux chunks (lors de l'indexation) et Ã  la requÃªte (lors de la recherche) :

```
Texte brut
    â”‚
    â–¼
1. Lowercase (si activÃ©)
    â”‚  "Les CONDITIONS de RÃ©siliation" â†’ "les conditions de rÃ©siliation"
    â–¼
2. Tokenization (split sur espaces + ponctuation)
    â”‚  â†’ ["les", "conditions", "de", "rÃ©siliation"]
    â–¼
3. Suppression des stopwords (si activÃ©)
    â”‚  â†’ ["conditions", "rÃ©siliation"]
    â–¼
4. Stemming (si activÃ©, Snowball)
    â”‚  â†’ ["condit", "rÃ©sili"]
    â–¼
5. N-grams (si > unigrams)
    â”‚  (1,2) â†’ ["condit", "rÃ©sili", "condit rÃ©sili"]
    â–¼
Tokens finaux
```

### 3.4 Stopwords multilingues

RAGKIT embarque des listes de stopwords pour les langues prises en charge :

| Langue | ClÃ© | Nombre de mots | Source |
|--------|-----|:-:|-------|
| FranÃ§ais | `french` | ~160 | NLTK / Snowball |
| Anglais | `english` | ~180 | NLTK / Snowball |
| Auto | `auto` | Variable | DÃ©tection de la langue du chunk via `doc_language` et application de la liste correspondante |

**Mode `auto`** :
- Si le chunk a une mÃ©tadonnÃ©e `doc_language`, la liste correspondante est utilisÃ©e.
- Si la langue n'est pas dÃ©tectÃ©e ou non supportÃ©e, aucun stopword n'est supprimÃ© (comportement conservateur).
- Les documents multilingues utilisent l'union des listes de stopwords des langues dÃ©tectÃ©es.

### 3.5 Stemming multilingue

Le stemming utilise l'algorithme **Snowball** (amÃ©liorÃ© par rapport Ã  Porter) :

| Langue | Stemmer | Exemple |
|--------|---------|---------|
| FranÃ§ais | `SnowballStemmer("french")` | "rÃ©siliation" â†’ "rÃ©sili", "conditions" â†’ "condit" |
| Anglais | `SnowballStemmer("english")` | "running" â†’ "run", "conditions" â†’ "condit" |
| Auto | DÃ©tection par `doc_language` | Applique le stemmer de la langue dÃ©tectÃ©e |

---

## 4. Catalogue complet des paramÃ¨tres RECHERCHE LEXICALE

### 4.1 ParamÃ¨tres principaux

| ParamÃ¨tre | ClÃ© config | Type | Min | Max | DÃ©faut | Description |
|-----------|------------|------|-----|-----|--------|-------------|
| ActivÃ©e | `retrieval.lexical.enabled` | bool | â€” | â€” | Selon profil | Activer/dÃ©sactiver la recherche lexicale |
| Algorithme | `retrieval.lexical.algorithm` | enum | â€” | â€” | `bm25` | `bm25` ou `bm25+` |
| Top K | `retrieval.lexical.top_k` | int | 1 | 100 | Selon profil | Nombre maximum de chunks retournÃ©s |
| Poids | `retrieval.lexical.weight` | float | 0.0 | 1.0 | Selon profil | Poids dans le score hybride (Ã‰tape 7) |

### 4.2 ParamÃ¨tres BM25

| ParamÃ¨tre | ClÃ© config | Type | Min | Max | DÃ©faut | Description |
|-----------|------------|------|-----|-----|--------|-------------|
| k1 | `retrieval.lexical.bm25_k1` | float | 0.5 | 3.0 | Selon profil | Saturation de la frÃ©quence des termes. k1 Ã©levÃ© = la rÃ©pÃ©tition compte davantage. |
| b | `retrieval.lexical.bm25_b` | float | 0.0 | 1.0 | Selon profil | Normalisation par longueur. b=1 = forte pÃ©nalitÃ© pour les documents longs. |
| delta | `retrieval.lexical.bm25_delta` | float | 0.0 | 2.0 | 0.5 | Bonus pour les termes prÃ©sents (BM25+ uniquement). |

### 4.3 ParamÃ¨tres de preprocessing

| ParamÃ¨tre | ClÃ© config | Type | DÃ©faut | Description |
|-----------|------------|------|--------|-------------|
| Lowercase | `retrieval.lexical.lowercase` | bool | `true` | Convertir en minuscules avant tokenization |
| Stopwords | `retrieval.lexical.remove_stopwords` | bool | `true` | Supprimer les mots vides |
| Langue stopwords | `retrieval.lexical.stopwords_lang` | enum | `auto` | `french` \| `english` \| `auto` |
| Stemming | `retrieval.lexical.stemming` | bool | `true` | Activer la racinisation (Snowball) |
| Langue stemmer | `retrieval.lexical.stemmer_lang` | enum | `auto` | `french` \| `english` \| `auto` |

### 4.4 ParamÃ¨tres avancÃ©s

| ParamÃ¨tre | ClÃ© config | Type | Min | Max | DÃ©faut | Description |
|-----------|------------|------|-----|-----|--------|-------------|
| Seuil de score | `retrieval.lexical.threshold` | float | 0.0 | â€” | 0.0 | Score BM25 minimum. 0.0 = pas de filtre. Les scores BM25 ne sont pas bornÃ©s Ã  1. |
| N-grams | `retrieval.lexical.ngram_range` | tuple | (1,1) | (1,3) | `(1,1)` | `(1,1)` = unigrams, `(1,2)` = uni+bigrams, `(1,3)` = uni+bi+trigrams |
| Debug | `retrieval.lexical.debug_default` | bool | â€” | â€” | `false` | Activer le mode debug par dÃ©faut |

### 4.5 VisibilitÃ© conditionnelle

| ParamÃ¨tre | Condition de visibilitÃ© |
|-----------|------------------------|
| `bm25_delta` | `algorithm == "bm25+"` |
| `stopwords_lang` | `remove_stopwords == true` |
| `stemmer_lang` | `stemming == true` |

### 4.6 RÃ©sumÃ© des impacts

| ParamÃ¨tre | Impact principal | Impact secondaire |
|-----------|-----------------|-------------------|
| `algorithm` | BM25+ donne de meilleurs scores pour les documents courts | Marginal sur les performances |
| `k1` | **IMPORTANT** â€” Poids de la rÃ©pÃ©tition des termes | k1 Ã©levÃ© favorise les chunks rÃ©pÃ©tant le mÃªme terme |
| `b` | **IMPORTANT** â€” PÃ©nalisation des documents longs | b=0 dÃ©sactive la normalisation de longueur |
| `lowercase` | Matching case-insensitive | Perte de la distinction entre acronymes et mots communs (ex: US vs us) |
| `remove_stopwords` | RÃ©duction du bruit (les mots courants ne polluent plus les scores) | RequÃªtes contenant uniquement des stopwords retournent 0 rÃ©sultat |
| `stemming` | Meilleur rappel (variantes morphologiques matchent) | Risque de faux positifs (ex: "universitÃ©" et "univers" â†’ mÃªme racine) |
| `ngram_range` | Capture des expressions multi-mots ("machine learning" matchÃ© comme bigram) | Augmente la taille de l'index et la latence |

---

## 5. Valeurs par dÃ©faut par profil

### 5.1 Matrice profil â†’ paramÃ¨tres de recherche lexicale

| ParamÃ¨tre | `technical_documentation` | `faq_support` | `legal_compliance` | `reports_analysis` | `general` |
|-----------|:---:|:---:|:---:|:---:|:---:|
| `enabled` | `true` | `false` | `true` | `true` | `true` |
| `algorithm` | `bm25` | `bm25` | `bm25` | `bm25` | `bm25` |
| `top_k` | 15 | 5 | 20 | 15 | 10 |
| `weight` | 0.5 | 0.0 | 0.5 | 0.4 | 0.5 |
| `bm25_k1` | 1.5 | 1.2 | 1.2 | 1.5 | 1.5 |
| `bm25_b` | 0.75 | 0.75 | 0.5 | 0.75 | 0.75 |
| `bm25_delta` | 0.5 | 0.5 | 0.5 | 0.5 | 0.5 |
| `lowercase` | `true` | `true` | `true` | `true` | `true` |
| `remove_stopwords` | `true` | `true` | `true` | `true` | `true` |
| `stopwords_lang` | `auto` | `auto` | `auto` | `auto` | `auto` |
| `stemming` | `true` | `true` | `true` | `true` | `true` |
| `stemmer_lang` | `auto` | `auto` | `auto` | `auto` | `auto` |
| `threshold` | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |
| `ngram_range` | `(1,1)` | `(1,1)` | `(1,1)` | `(1,1)` | `(1,1)` |
| `debug_default` | `false` | `false` | `false` | `false` | `false` |

### 5.2 Justification des choix

- **`faq_support` â†’ `enabled=false`, `weight=0.0`** : le profil FAQ est orientÃ© sÃ©mantique pur (les questions des utilisateurs sont rarement formulÃ©es avec les mots exacts de la rÃ©ponse). La recherche lexicale est dÃ©sactivÃ©e par dÃ©faut mais peut Ãªtre activÃ©e manuellement.
- **`technical_documentation` â†’ `bm25_k1=1.5`** : les documents techniques contiennent souvent des termes rÃ©pÃ©tÃ©s (noms de fonction, codes produit). Un k1 modÃ©rÃ©ment Ã©levÃ© valorise cette rÃ©pÃ©tition.
- **`legal_compliance` â†’ `bm25_b=0.5`** : les textes juridiques sont naturellement longs. Une normalisation de longueur rÃ©duite (`b=0.5`) Ã©vite de trop pÃ©naliser les chunks longs qui sont pertinents en contexte juridique.
- **`legal_compliance` â†’ `bm25_k1=1.2`** : les textes juridiques utilisent un vocabulaire prÃ©cis mais sans rÃ©pÃ©tition excessive. Un k1 plus faible est appropriÃ©.
- **Tous `stemming=true`** : le stemming amÃ©liore significativement le rappel en franÃ§ais (conjugaisons, accords, dÃ©rivations) avec un risque de faux positifs acceptable.
- **Tous `ngram_range=(1,1)`** : les unigrams suffisent pour la majoritÃ© des cas d'usage. Les bigrams/trigrams augmentent la taille de l'index et ne sont nÃ©cessaires que pour des cas spÃ©cialisÃ©s (expressions figÃ©es, terminologie composite).

---

## 6. SpÃ©cifications techniques

### 6.1 SchÃ©ma Pydantic (backend)

```python
# ragkit/config/lexical_schema.py
"""Pydantic schemas for lexical (BM25) search configuration."""

from __future__ import annotations

from enum import Enum
from typing import Tuple

from pydantic import BaseModel, Field


class BM25Algorithm(str, Enum):
    BM25 = "bm25"
    BM25_PLUS = "bm25+"


class StopwordsLang(str, Enum):
    FRENCH = "french"
    ENGLISH = "english"
    AUTO = "auto"


class LexicalSearchConfig(BaseModel):
    """Lexical (BM25) search configuration."""

    enabled: bool = True
    algorithm: BM25Algorithm = BM25Algorithm.BM25
    top_k: int = Field(default=10, ge=1, le=100)
    weight: float = Field(default=0.5, ge=0.0, le=1.0,
        description="Weight in hybrid search (Ã‰tape 7)")

    # BM25 parameters
    bm25_k1: float = Field(default=1.5, ge=0.5, le=3.0)
    bm25_b: float = Field(default=0.75, ge=0.0, le=1.0)
    bm25_delta: float = Field(default=0.5, ge=0.0, le=2.0,
        description="BM25+ delta (only used when algorithm=bm25+)")

    # Preprocessing
    lowercase: bool = True
    remove_stopwords: bool = True
    stopwords_lang: StopwordsLang = StopwordsLang.AUTO
    stemming: bool = True
    stemmer_lang: StopwordsLang = StopwordsLang.AUTO

    # Advanced
    threshold: float = Field(default=0.0, ge=0.0)
    ngram_range: tuple[int, int] = Field(default=(1, 1))
    debug_default: bool = False
```

### 6.2 Moteur de recherche BM25 (backend)

```python
# ragkit/retrieval/lexical_engine.py
"""Lexical search engine â€” BM25/BM25+ with configurable preprocessing."""

from __future__ import annotations

import math
import time
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from pathlib import Path

from ragkit.config.lexical_schema import LexicalSearchConfig, BM25Algorithm


@dataclass
class BM25SearchResult:
    """A single BM25 search result."""
    chunk_id: str
    score: float
    text: str
    metadata: dict
    matched_terms: dict[str, int]     # term â†’ occurrence count in chunk
    doc_title: str | None = None
    doc_path: str | None = None
    doc_type: str | None = None
    page_number: int | None = None
    chunk_index: int | None = None
    chunk_total: int | None = None
    section_header: str | None = None
    doc_language: str | None = None
    category: str | None = None
    keywords: list[str] = field(default_factory=list)


@dataclass
class LexicalDebugInfo:
    """Debug information for a lexical search query."""
    query_text: str
    query_tokens: list[str]
    tokenization_latency_ms: int
    search_latency_ms: int
    total_latency_ms: int
    results_from_index: int
    results_after_threshold: int
    index_stats: dict          # documents, unique_terms, size_bytes


@dataclass
class LexicalSearchResponse:
    """Complete response from a lexical search."""
    query: str
    results: list[BM25SearchResult]
    total_results: int
    debug: LexicalDebugInfo | None = None


class TextPreprocessor:
    """Text preprocessing pipeline for BM25 tokenization."""

    def __init__(self, config: LexicalSearchConfig):
        self.config = config
        self._stopwords: dict[str, set[str]] = {}
        self._stemmers: dict[str, object] = {}
        self._load_resources()

    def _load_resources(self) -> None:
        """Load stopwords lists and stemmers."""
        from nltk.corpus import stopwords as nltk_stopwords
        from nltk.stem.snowball import SnowballStemmer

        for lang in ("french", "english"):
            try:
                self._stopwords[lang] = set(nltk_stopwords.words(lang))
            except LookupError:
                import nltk
                nltk.download("stopwords", quiet=True)
                self._stopwords[lang] = set(nltk_stopwords.words(lang))
            self._stemmers[lang] = SnowballStemmer(lang)

    def tokenize(
        self, text: str, language: str | None = None
    ) -> list[str]:
        """Full preprocessing pipeline: lowercase â†’ tokenize â†’ stopwords â†’ stemming â†’ ngrams."""
        import re

        # 1. Lowercase
        if self.config.lowercase:
            text = text.lower()

        # 2. Tokenize (split on non-alphanumeric, keep numbers)
        tokens = re.findall(r"\b\w+\b", text)

        # 3. Remove stopwords
        if self.config.remove_stopwords:
            lang = self._resolve_lang(
                self.config.stopwords_lang.value, language)
            if lang and lang in self._stopwords:
                sw = self._stopwords[lang]
                tokens = [t for t in tokens if t not in sw]

        # 4. Stemming
        if self.config.stemming:
            lang = self._resolve_lang(
                self.config.stemmer_lang.value, language)
            if lang and lang in self._stemmers:
                stemmer = self._stemmers[lang]
                tokens = [stemmer.stem(t) for t in tokens]

        # 5. N-grams
        min_n, max_n = self.config.ngram_range
        if max_n > 1:
            ngrams = []
            for n in range(min_n, max_n + 1):
                for i in range(len(tokens) - n + 1):
                    ngrams.append(" ".join(tokens[i:i + n]))
            return ngrams

        return tokens

    def _resolve_lang(
        self, config_lang: str, doc_lang: str | None
    ) -> str | None:
        if config_lang != "auto":
            return config_lang
        if doc_lang:
            mapping = {"fr": "french", "en": "english"}
            return mapping.get(doc_lang[:2])
        return None


class BM25Index:
    """In-memory BM25 index with persistence."""

    def __init__(self, preprocessor: TextPreprocessor):
        self.preprocessor = preprocessor
        self._doc_ids: list[str] = []
        self._doc_lengths: list[int] = []
        self._avg_dl: float = 0.0
        self._doc_freqs: dict[str, int] = defaultdict(int)  # term â†’ df
        self._term_freqs: dict[str, dict[str, int]] = {}    # doc_id â†’ {term: tf}
        self._doc_texts: dict[str, str] = {}
        self._doc_metadata: dict[str, dict] = {}
        self._doc_languages: dict[str, str | None] = {}

    @property
    def num_documents(self) -> int:
        return len(self._doc_ids)

    @property
    def num_unique_terms(self) -> int:
        return len(self._doc_freqs)

    def add_document(
        self,
        doc_id: str,
        text: str,
        metadata: dict,
        language: str | None = None,
    ) -> None:
        """Add a document (chunk) to the index."""
        tokens = self.preprocessor.tokenize(text, language)
        tf = Counter(tokens)

        self._doc_ids.append(doc_id)
        self._doc_lengths.append(len(tokens))
        self._term_freqs[doc_id] = dict(tf)
        self._doc_texts[doc_id] = text
        self._doc_metadata[doc_id] = metadata
        self._doc_languages[doc_id] = language

        for term in tf:
            self._doc_freqs[term] += 1

        # Recompute average document length
        self._avg_dl = (
            sum(self._doc_lengths) / len(self._doc_lengths)
            if self._doc_lengths else 0.0
        )

    def remove_document(self, doc_id: str) -> None:
        """Remove a document from the index."""
        if doc_id not in self._term_freqs:
            return

        tf = self._term_freqs.pop(doc_id)
        for term in tf:
            self._doc_freqs[term] -= 1
            if self._doc_freqs[term] <= 0:
                del self._doc_freqs[term]

        idx = self._doc_ids.index(doc_id)
        self._doc_ids.pop(idx)
        self._doc_lengths.pop(idx)
        self._doc_texts.pop(doc_id, None)
        self._doc_metadata.pop(doc_id, None)
        self._doc_languages.pop(doc_id, None)

        self._avg_dl = (
            sum(self._doc_lengths) / len(self._doc_lengths)
            if self._doc_lengths else 0.0
        )

    def search(
        self,
        query: str,
        config: LexicalSearchConfig,
        language: str | None = None,
        filter_conditions: dict | None = None,
    ) -> list[tuple[str, float, dict[str, int]]]:
        """Search the index. Returns list of (doc_id, score, matched_terms)."""
        query_tokens = self.preprocessor.tokenize(query, language)
        if not query_tokens:
            return []

        N = self.num_documents
        scores: dict[str, float] = {}
        matched: dict[str, dict[str, int]] = defaultdict(dict)

        for token in query_tokens:
            df = self._doc_freqs.get(token, 0)
            if df == 0:
                continue

            idf = math.log((N - df + 0.5) / (df + 0.5) + 1)

            for doc_id in self._doc_ids:
                tf = self._term_freqs.get(doc_id, {}).get(token, 0)
                if tf == 0:
                    continue

                # Apply metadata filters
                if filter_conditions and not self._matches_filters(
                    doc_id, filter_conditions
                ):
                    continue

                doc_len = self._doc_lengths[self._doc_ids.index(doc_id)]
                k1 = config.bm25_k1
                b = config.bm25_b

                numerator = tf * (k1 + 1)
                denominator = tf + k1 * (1 - b + b * doc_len / self._avg_dl)

                if config.algorithm == BM25Algorithm.BM25_PLUS:
                    term_score = idf * (numerator / denominator + config.bm25_delta)
                else:
                    term_score = idf * (numerator / denominator)

                scores[doc_id] = scores.get(doc_id, 0.0) + term_score
                matched[doc_id][token] = tf

        # Sort by score descending
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [
            (doc_id, score, matched.get(doc_id, {}))
            for doc_id, score in ranked
        ]

    def _matches_filters(
        self, doc_id: str, filter_conditions: dict
    ) -> bool:
        """Check if a document matches the filter conditions."""
        meta = self._doc_metadata.get(doc_id, {})
        for field, condition in filter_conditions.items():
            if "$in" in condition:
                if meta.get(field) not in condition["$in"]:
                    return False
        return True

    def save(self, path: Path) -> None:
        """Persist the index to disk."""
        import json
        path.mkdir(parents=True, exist_ok=True)
        data = {
            "doc_ids": self._doc_ids,
            "doc_lengths": self._doc_lengths,
            "avg_dl": self._avg_dl,
            "doc_freqs": dict(self._doc_freqs),
            "term_freqs": self._term_freqs,
            "doc_texts": self._doc_texts,
            "doc_metadata": self._doc_metadata,
            "doc_languages": self._doc_languages,
        }
        (path / "bm25_index.json").write_text(
            json.dumps(data, ensure_ascii=False), encoding="utf-8"
        )

    def load(self, path: Path) -> bool:
        """Load the index from disk. Returns True if loaded."""
        import json
        index_file = path / "bm25_index.json"
        if not index_file.exists():
            return False
        data = json.loads(index_file.read_text(encoding="utf-8"))
        self._doc_ids = data["doc_ids"]
        self._doc_lengths = data["doc_lengths"]
        self._avg_dl = data["avg_dl"]
        self._doc_freqs = defaultdict(int, data["doc_freqs"])
        self._term_freqs = data["term_freqs"]
        self._doc_texts = data["doc_texts"]
        self._doc_metadata = data["doc_metadata"]
        self._doc_languages = data.get("doc_languages", {})
        return True


class LexicalSearchEngine:
    """Orchestrates lexical search with preprocessing and scoring."""

    def __init__(
        self,
        config: LexicalSearchConfig,
        index: BM25Index,
    ):
        self.config = config
        self.index = index

    async def search(
        self,
        query: str,
        top_k: int | None = None,
        threshold: float | None = None,
        filters: dict | None = None,
        include_debug: bool = False,
    ) -> LexicalSearchResponse:
        """Execute a lexical search query."""
        _top_k = top_k or self.config.top_k
        _threshold = threshold if threshold is not None else self.config.threshold

        t_start = time.perf_counter()

        # 1. Tokenize query
        t_tok_start = time.perf_counter()
        query_tokens = self.index.preprocessor.tokenize(query)
        t_tok = time.perf_counter() - t_tok_start

        # 2. BM25 search
        t_search_start = time.perf_counter()
        raw_results = self.index.search(
            query, self.config, filter_conditions=filters
        )
        t_search = time.perf_counter() - t_search_start

        # 3. Apply threshold
        filtered = [
            (doc_id, score, terms)
            for doc_id, score, terms in raw_results
            if score >= _threshold
        ]

        # 4. Truncate to top_k
        final = filtered[:_top_k]

        t_total = time.perf_counter() - t_start

        # 5. Build response
        results = [
            self._to_result(doc_id, score, terms)
            for doc_id, score, terms in final
        ]

        debug = None
        if include_debug:
            debug = LexicalDebugInfo(
                query_text=query,
                query_tokens=query_tokens,
                tokenization_latency_ms=int(t_tok * 1000),
                search_latency_ms=int(t_search * 1000),
                total_latency_ms=int(t_total * 1000),
                results_from_index=len(raw_results),
                results_after_threshold=len(filtered),
                index_stats={
                    "documents": self.index.num_documents,
                    "unique_terms": self.index.num_unique_terms,
                },
            )

        return LexicalSearchResponse(
            query=query,
            results=results,
            total_results=len(final),
            debug=debug,
        )

    def _to_result(
        self, doc_id: str, score: float, matched_terms: dict[str, int]
    ) -> BM25SearchResult:
        """Convert index result to search result."""
        text = self.index._doc_texts.get(doc_id, "")
        meta = self.index._doc_metadata.get(doc_id, {})
        return BM25SearchResult(
            chunk_id=doc_id,
            score=score,
            text=text,
            metadata=meta,
            matched_terms=matched_terms,
            doc_title=meta.get("doc_title"),
            doc_path=meta.get("doc_path"),
            doc_type=meta.get("doc_type"),
            page_number=meta.get("page_number"),
            chunk_index=meta.get("chunk_index"),
            chunk_total=meta.get("chunk_total"),
            section_header=meta.get("section_header"),
            doc_language=meta.get("doc_language"),
            category=meta.get("category"),
            keywords=meta.get("keywords", []),
        )
```

### 6.3 Extension du pipeline d'ingestion

Le `IngestionOrchestrator` (Ã‰tape 4) est Ã©tendu pour alimenter l'index BM25 :

```python
# ragkit/ingestion/orchestrator.py â€” modifications Ã‰tape 6

class IngestionOrchestrator:
    def __init__(
        self,
        parser, chunker, embedder, store,
        registry,
        bm25_index: BM25Index,  # â† NOUVEAU
    ):
        ...
        self.bm25_index = bm25_index

    async def _process_document(self, file_path: Path) -> DocumentResult:
        # 1. Parse
        doc_content = await self.parser.parse(file_path)
        # 2. Chunk
        chunks = self.chunker.chunk(doc_content.text, doc_content.metadata)
        # 3. Embed
        texts = [c.text for c in chunks]
        vectors = await self.embedder.embed_texts(texts)
        # 4. Store vectors
        points = [...]
        await self.store.upsert(points)

        # 5. Index in BM25 â† NOUVEAU
        for i, chunk in enumerate(chunks):
            chunk_id = self._make_point_id(file_path, i)
            self.bm25_index.add_document(
                doc_id=chunk_id,
                text=chunk.text,
                metadata=self._make_payload(doc_content, chunk, i, len(chunks)),
                language=doc_content.metadata.get("language"),
            )

        return DocumentResult(...)

    async def _remove_document(self, doc_id: str) -> None:
        """Remove a document from both vector store and BM25 index."""
        await self.store.delete_by_doc_id(doc_id)
        # Remove all chunks for this doc from BM25 index
        chunks_to_remove = [
            cid for cid in self.bm25_index._doc_ids
            if self.bm25_index._doc_metadata.get(cid, {}).get("doc_id") == doc_id
        ]
        for chunk_id in chunks_to_remove:
            self.bm25_index.remove_document(chunk_id)

    async def _finalize(self) -> None:
        """Save BM25 index after ingestion."""
        index_path = Path("~/.ragkit/data/bm25_index/").expanduser()
        self.bm25_index.save(index_path)
```

### 6.4 API REST (routes backend)

#### 6.4.1 Routes Config

| Endpoint | MÃ©thode | Description | Corps | RÃ©ponse |
|----------|---------|-------------|-------|---------|
| `/api/retrieval/lexical/config` | GET | Config recherche lexicale courante | â€” | `LexicalSearchConfig` |
| `/api/retrieval/lexical/config` | PUT | Met Ã  jour la config | `LexicalSearchConfig` (partiel) | `LexicalSearchConfig` |
| `/api/retrieval/lexical/config/reset` | POST | RÃ©initialise au profil actif | â€” | `LexicalSearchConfig` |

#### 6.4.2 Routes Recherche

| Endpoint | MÃ©thode | Description | Corps | RÃ©ponse |
|----------|---------|-------------|-------|---------|
| `/api/search/lexical` | POST | ExÃ©cute une recherche BM25 | `LexicalSearchQuery` | `LexicalSearchResponse` |

#### 6.4.3 Routes Index BM25

| Endpoint | MÃ©thode | Description | Corps | RÃ©ponse |
|----------|---------|-------------|-------|---------|
| `/api/retrieval/lexical/index/stats` | GET | Statistiques de l'index BM25 | â€” | `BM25IndexStats` |
| `/api/retrieval/lexical/index/rebuild` | POST | Reconstruit l'index Ã  partir des chunks existants | â€” | `{ status: string, duration_s: float }` |

#### 6.4.4 ModÃ¨les de requÃªte et rÃ©ponse

```python
class LexicalSearchQuery(BaseModel):
    """Lexical search query from the chat interface."""
    query: str = Field(..., min_length=1, max_length=2000)
    top_k: int | None = None
    threshold: float | None = None
    filters: SearchFilters | None = None   # Shared with semantic (Ã‰tape 5)
    include_debug: bool = False
    page: int = Field(default=1, ge=1)
    page_size: int = Field(default=5, ge=1, le=50)

class LexicalSearchResponseAPI(BaseModel):
    query: str
    results: list[LexicalSearchResultItem]
    total_results: int
    page: int
    page_size: int
    has_more: bool
    debug: LexicalDebugInfo | None = None

class LexicalSearchResultItem(BaseModel):
    chunk_id: str
    score: float                   # BM25 score (non bornÃ©)
    text: str
    text_preview: str              # TronquÃ© Ã  300 caractÃ¨res
    matched_terms: dict[str, int]  # terme â†’ nombre d'occurrences
    highlight_positions: list[dict] # [{start, end, term}] pour le surlignage
    doc_title: str | None
    doc_path: str | None
    doc_type: str | None
    page_number: int | None
    chunk_index: int | None
    chunk_total: int | None
    chunk_tokens: int | None
    section_header: str | None
    doc_language: str | None
    category: str | None
    keywords: list[str]
    ingestion_version: str | None

class BM25IndexStats(BaseModel):
    num_documents: int
    num_unique_terms: int
    size_bytes: int
    last_updated_version: str | None
    last_updated_at: str | None
```

### 6.5 Commandes Tauri (Rust) â€” ajouts

```rust
// desktop/src-tauri/src/commands.rs (ajouts Ã‰tape 6)

// Lexical config
#[tauri::command]
pub async fn get_lexical_search_config() -> Result<serde_json::Value, String> { ... }

#[tauri::command]
pub async fn update_lexical_search_config(config: serde_json::Value) -> Result<serde_json::Value, String> { ... }

#[tauri::command]
pub async fn reset_lexical_search_config() -> Result<serde_json::Value, String> { ... }

// Lexical search
#[tauri::command]
pub async fn lexical_search(query: serde_json::Value) -> Result<serde_json::Value, String> { ... }

// BM25 index
#[tauri::command]
pub async fn get_bm25_index_stats() -> Result<serde_json::Value, String> { ... }

#[tauri::command]
pub async fn rebuild_bm25_index() -> Result<serde_json::Value, String> { ... }
```

### 6.6 Composants React â€” arborescence

```
desktop/src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ settings/
â”‚   â”‚   â”œâ”€â”€ LexicalSearchSettings.tsx          â† NOUVEAU : section complÃ¨te
â”‚   â”‚   â”œâ”€â”€ BM25ParamsPanel.tsx                â† NOUVEAU : k1, b, delta
â”‚   â”‚   â”œâ”€â”€ LexicalPreprocessingPanel.tsx      â† NOUVEAU : lowercase, stopwords, stemming
â”‚   â”‚   â”œâ”€â”€ BM25IndexStatusPanel.tsx           â† NOUVEAU : Ã©tat de l'index
â”‚   â”‚   â””â”€â”€ ... (existants)
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ SearchModeSelector.tsx             â† NOUVEAU : sÃ©lecteur sÃ©mantique/lexicale
â”‚   â”‚   â”œâ”€â”€ LexicalResultCard.tsx              â† NOUVEAU : carte rÃ©sultat avec termes matchÃ©s
â”‚   â”‚   â”œâ”€â”€ TermHighlighter.tsx                â† NOUVEAU : surlignage des termes matchÃ©s
â”‚   â”‚   â”œâ”€â”€ MatchedTermsBadge.tsx              â† NOUVEAU : compteur de termes matchÃ©s
â”‚   â”‚   â””â”€â”€ ... (existants Ã‰tape 5)
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ ... (existants)
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useLexicalSearchConfig.ts              â† NOUVEAU : hook config
â”‚   â”œâ”€â”€ useLexicalSearch.ts                    â† NOUVEAU : hook exÃ©cution recherche
â”‚   â”œâ”€â”€ useBM25IndexStats.ts                   â† NOUVEAU : hook stats index
â”‚   â””â”€â”€ ... (existants)
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ ipc.ts                                 â† MODIFIER : ajouter routes lexical
â””â”€â”€ locales/
    â”œâ”€â”€ fr.json                                â† MODIFIER : ajouter clÃ©s lexical
    â””â”€â”€ en.json                                â† MODIFIER : ajouter clÃ©s lexical
```

### 6.7 DÃ©tail du composant `SearchModeSelector.tsx`

```tsx
interface SearchMode {
  id: "semantic" | "lexical";
  icon: string;
  label: string;
  description: string;
}

const SEARCH_MODES: SearchMode[] = [
  {
    id: "semantic",
    icon: "ğŸ”",
    label: "SÃ©mantique",
    description: "Recherche par sens et concepts",
  },
  {
    id: "lexical",
    icon: "ğŸ“",
    label: "Lexicale",
    description: "Recherche par mots-clÃ©s exacts (BM25)",
  },
  // L'option "Hybride" sera ajoutÃ©e Ã  l'Ã‰tape 7
];

export function SearchModeSelector({
  mode,
  onModeChange,
  lexicalEnabled,   // from config
  semanticEnabled,  // from config
}: SearchModeSelectorProps) {
  const availableModes = SEARCH_MODES.filter((m) => {
    if (m.id === "lexical") return lexicalEnabled;
    if (m.id === "semantic") return semanticEnabled;
    return true;
  });

  return (
    <select
      value={mode}
      onChange={(e) => onModeChange(e.target.value as SearchMode["id"])}
      className="..."
    >
      {availableModes.map((m) => (
        <option key={m.id} value={m.id} title={m.description}>
          {m.icon} {m.label}
        </option>
      ))}
    </select>
  );
}
```

### 6.8 DÃ©tail du composant `TermHighlighter.tsx`

```tsx
interface TermHighlighterProps {
  text: string;
  matchedTerms: Record<string, number>;
  stemming: boolean;
}

export function TermHighlighter({
  text, matchedTerms, stemming
}: TermHighlighterProps) {
  // Build regex pattern from matched terms
  // If stemming is active, also find morphological variants
  const terms = Object.keys(matchedTerms);
  if (terms.length === 0) return <span>{text}</span>;

  const pattern = new RegExp(
    `\\b(${terms.map(escapeRegex).join("|")})\\w*\\b`,
    "gi"
  );

  const parts = text.split(pattern);

  return (
    <span>
      {parts.map((part, i) =>
        pattern.test(part) ? (
          <mark key={i} className="bg-yellow-200 font-semibold px-0.5 rounded">
            {part}
          </mark>
        ) : (
          <span key={i}>{part}</span>
        )
      )}
    </span>
  );
}
```

### 6.9 Persistance

La config de recherche lexicale est stockÃ©e dans `settings.json` :

```json
{
  "retrieval": {
    "architecture": "hybrid_rerank",
    "semantic": { "...": "..." },
    "lexical": {
      "enabled": true,
      "algorithm": "bm25",
      "top_k": 15,
      "weight": 0.5,
      "bm25_k1": 1.5,
      "bm25_b": 0.75,
      "bm25_delta": 0.5,
      "lowercase": true,
      "remove_stopwords": true,
      "stopwords_lang": "auto",
      "stemming": true,
      "stemmer_lang": "auto",
      "threshold": 0.0,
      "ngram_range": [1, 1],
      "debug_default": false
    },
    "hybrid": { "...": "..." }
  }
}
```

L'index BM25 est stockÃ© sÃ©parÃ©ment dans `~/.ragkit/data/bm25_index/bm25_index.json`.

### 6.10 DÃ©pendances Python ajoutÃ©es

```toml
# pyproject.toml â€” ajouts aux dependencies pour Ã‰tape 6
dependencies = [
    # ... (existants Ã‰tapes 0-5)
    "nltk>=3.8",                    # Stopwords lists + Snowball stemmer
]
```

**Post-installation** : un script de setup tÃ©lÃ©charge les ressources NLTK nÃ©cessaires :

```python
# ragkit/setup_nltk.py
import nltk
nltk.download("stopwords", quiet=True)
nltk.download("punkt", quiet=True)
```

Ce script est appelÃ© au premier dÃ©marrage de l'application ou lors de l'activation de la recherche lexicale.

---

## 7. CritÃ¨res d'acceptation

### 7.1 Fonctionnels

| # | CritÃ¨re |
|---|---------|
| F1 | La section `PARAMÃˆTRES > ParamÃ¨tres avancÃ©s > RECHERCHE LEXICALE` est accessible et affiche tous les paramÃ¨tres |
| F2 | Le toggle `enabled` active/dÃ©sactive la recherche lexicale |
| F3 | Le sÃ©lecteur d'algorithme propose BM25 et BM25+ |
| F4 | Le paramÃ¨tre `delta` n'est visible que lorsque l'algorithme est BM25+ |
| F5 | Les sliders k1 et b modifient les paramÃ¨tres BM25 avec validation des bornes |
| F6 | Les toggles de preprocessing (lowercase, stopwords, stemming) sont fonctionnels |
| F7 | Les dropdowns de langue (stopwords, stemmer) apparaissent conditionnellement |
| F8 | Le panneau "Ã‰tat de l'index BM25" affiche les statistiques de l'index |
| F9 | Le bouton "Reconstruire l'index" reconstruit l'index sans relancer l'ingestion complÃ¨te |
| F10 | Le sÃ©lecteur de mode de recherche est visible dans le CHAT |
| F11 | Le mode "Lexicale" effectue une recherche BM25 et affiche les rÃ©sultats |
| F12 | Les termes matchÃ©s sont surlignÃ©s dans les rÃ©sultats lexicaux |
| F13 | Le compteur "Termes matchÃ©s" affiche chaque terme avec son nombre d'occurrences |
| F14 | Les rÃ©sultats lexicaux affichent le score BM25 (non bornÃ©) avec coloration relative |
| F15 | Les filtres rapides (documents, langues, types, catÃ©gories) fonctionnent en mode lexical |
| F16 | Le mode debug affiche les tokens de la requÃªte, les latences et les stats de l'index |
| F17 | Le sÃ©lecteur de mode ne propose que les modes activÃ©s (si lexical `enabled=false`, l'option est grisÃ©e) |
| F18 | L'index BM25 est construit automatiquement lors de l'ingestion (Ã‰tape 4) |
| F19 | Le badge "ModifiÃ©" apparaÃ®t Ã  cÃ´tÃ© de chaque paramÃ¨tre modifiÃ© |
| F20 | Le bouton "RÃ©initialiser au profil" restaure les valeurs par dÃ©faut |
| F21 | Tous les textes sont traduits FR/EN via i18n |

### 7.2 Techniques

| # | CritÃ¨re |
|---|---------|
| T1 | `GET /api/retrieval/lexical/config` retourne la config courante |
| T2 | `PUT /api/retrieval/lexical/config` valide et persiste les modifications |
| T3 | `POST /api/retrieval/lexical/config/reset` restaure les valeurs du profil actif |
| T4 | `POST /api/search/lexical` retourne les rÃ©sultats de la recherche BM25 |
| T5 | L'algorithme BM25 classique produit des scores conformes Ã  la formule documentÃ©e |
| T6 | L'algorithme BM25+ produit des scores avec le bonus delta |
| T7 | Le preprocessing lowercase fonctionne correctement |
| T8 | Le retrait des stopwords franÃ§ais fonctionne (vÃ©rifiÃ© avec "le", "la", "de", "et") |
| T9 | Le retrait des stopwords anglais fonctionne (vÃ©rifiÃ© avec "the", "a", "is", "and") |
| T10 | Le stemming franÃ§ais fonctionne ("rÃ©siliation" â†’ "rÃ©sili", "courant" â†’ "cour") |
| T11 | Le stemming anglais fonctionne ("running" â†’ "run", "conditions" â†’ "condit") |
| T12 | Le mode `auto` pour les langues utilise la mÃ©tadonnÃ©e `doc_language` du chunk |
| T13 | L'index BM25 est alimentÃ© automatiquement pendant l'ingestion |
| T14 | L'index BM25 est persistÃ© dans `~/.ragkit/data/bm25_index/` |
| T15 | L'index supporte l'ajout et la suppression incrÃ©mentale de documents |
| T16 | Le filtrage par mÃ©tadonnÃ©es fonctionne dans la recherche BM25 |
| T17 | Le seuil de score filtre correctement les rÃ©sultats |
| T18 | `GET /api/retrieval/lexical/index/stats` retourne les statistiques de l'index |
| T19 | `POST /api/retrieval/lexical/index/rebuild` reconstruit l'index Ã  partir des textes des chunks existants |
| T20 | La latence d'une recherche BM25 est < 100 ms pour un index de 10K chunks |
| T21 | La config recherche lexicale est persistÃ©e dans `settings.json` sous `retrieval.lexical` |
| T22 | `tsc --noEmit` ne produit aucune erreur TypeScript |
| T23 | Le CI passe sur les 4 targets (lint + build) |

---

## 8. PÃ©rimÃ¨tre exclus (Ã‰tape 6)

- **Recherche hybride** (fusion sÃ©mantique + lexicale) : sera ajoutÃ©e Ã  l'Ã‰tape 7.
- **Reranking** : sera ajoutÃ© Ã  l'Ã‰tape 8.
- **GÃ©nÃ©ration LLM** : sera ajoutÃ©e Ã  l'Ã‰tape 9. Le chat affiche uniquement les rÃ©sultats bruts.
- **Lemmatisation** (alternative au stemming plus prÃ©cise mais plus lente) : amÃ©lioration future. Le stemming Snowball est suffisant pour la V1.
- **Index BM25 distribuÃ©** (Elasticsearch, Solr) : non pertinent pour une application desktop locale. L'index en mÃ©moire avec persistance JSON est suffisant pour les volumes visÃ©s (<100K chunks).
- **Quantification TF-IDF** (alternative Ã  BM25) : BM25 est strictement supÃ©rieur Ã  TF-IDF pour le RAG.
- **Custom stopwords** (liste personnalisÃ©e par l'utilisateur) : amÃ©lioration future.
- **N-grams > 3** : pas de cas d'usage identifiÃ©.
- **Sparse embeddings** (SPLADE, ColBERT) : amÃ©lioration future, au-delÃ  du scope de la V1.

---

## 9. Estimation

| TÃ¢che | Effort estimÃ© |
|-------|---------------|
| SchÃ©ma Pydantic `LexicalSearchConfig` + validation | 0.5 jour |
| `TextPreprocessor` (tokenization, stopwords, stemming, n-grams) | 1.5 jours |
| `BM25Index` (add/remove/search, BM25 + BM25+, persistence JSON) | 2.5 jours |
| `LexicalSearchEngine` (orchestration, filtres, debug) | 1 jour |
| Extension `IngestionOrchestrator` (alimentation BM25 en parallÃ¨le) | 1 jour |
| Reconstruction d'index (`rebuild`) Ã  partir des chunks existants | 0.5 jour |
| Routes API config (CRUD) | 0.5 jour |
| Routes API recherche (`/api/search/lexical`) + pagination | 0.5 jour |
| Routes API index (`stats`, `rebuild`) | 0.5 jour |
| Commandes Tauri (Rust) | 0.5 jour |
| Composant `LexicalSearchSettings.tsx` (section paramÃ¨tres) | 1 jour |
| Composants `BM25ParamsPanel.tsx`, `LexicalPreprocessingPanel.tsx`, `BM25IndexStatusPanel.tsx` | 1 jour |
| Composant `SearchModeSelector.tsx` (sÃ©lecteur dans le chat) | 0.5 jour |
| Composant `LexicalResultCard.tsx` + `TermHighlighter.tsx` + `MatchedTermsBadge.tsx` | 1.5 jours |
| Modification `ChatView.tsx` (dispatch sÃ©mantique vs lexical) | 0.5 jour |
| Hooks (`useLexicalSearchConfig`, `useLexicalSearch`, `useBM25IndexStats`) | 0.5 jour |
| Traductions i18n (FR + EN) â€” lexical settings + chat labels | 0.5 jour |
| Setup NLTK (download automatique stopwords + punkt au premier lancement) | 0.5 jour |
| Tests unitaires `TextPreprocessor` (tokenization, stopwords FR/EN, stemming FR/EN, n-grams) | 1 jour |
| Tests unitaires `BM25Index` (add, remove, search, BM25 vs BM25+, persistence) | 1.5 jours |
| Tests unitaires `LexicalSearchEngine` (filtres, seuil, debug) | 0.5 jour |
| Tests d'intÃ©gration (ingestion â†’ index BM25 â†’ recherche â†’ rÃ©sultats) | 1 jour |
| Tests manuels + corrections | 1 jour |
| **Total** | **~19 jours** |
